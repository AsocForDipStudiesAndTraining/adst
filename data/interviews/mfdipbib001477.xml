<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="mfdipbib001477">
<teiHeader>
<fileDesc>
<titleStmt>

<title type="interview">Interview with Nena Vreeland</title>
<title type="series">The Foreign Affairs Oral History Collection of the Association for Diplomatic Studies and Training</title>
<respStmt>
<resp>Selected and converted.</resp>
<name>American Memory, Library of Congress</name>
</respStmt>
</titleStmt>
<publicationStmt>
<p>Washington, D.C., 2007</p>
</publicationStmt>
<sourceDesc>
                <p type="lccn"/>
                <p type="sourcecol">Manuscript Division, Library of Congress</p>
                <p type="copyright">Copyright status not determined; refer to accompanying matter.</p>
            </sourceDesc>
</fileDesc>
<encodingDesc>
<projectDesc>
<p>The Library of Congress makes digitized historical materials available for education and scholarship.</p>
</projectDesc>
<editorialDecl>
<p>This transcription is intended to have an accuracy rate of 99.95 percent or greater.</p>
</editorialDecl>


</encodingDesc>
<revisionDesc>
            <listChange>
                <change type="encoding" when="2007-11-05"/>
                <change type="rev" when="2007-11-05"/>
                <change type="rev" when="2017-02-07" who="WicentowskiJC@state.gov">Migrate from AMMEM2.DTD to basic TEI P5</change>
            </listChange>
        </revisionDesc>
    </teiHeader>
<text>
<pb facs="0001" n="Page0001"/>
<body>
<div>
<p>Foreign Affairs Oral History Collection Association for Diplomatic Studies and Training United States Foreign Assistance Oral History Program </p>
<p>NENA VREELAND</p>
<p>Interviewed by W. Haven North</p>
<p>Initial interview date April 28, 1998</p>
<p>
<hi rend="italics">Q:  Today is April 28, 1998.  The interview is with Nena Vreeland who served with AID —  for how many years?</hi>
</p>
<p>VREELAND:  Seventeen years.</p>
<p>
<hi rend="italics">Q:  What years were these?</hi>
</p>
<p>VREELAND:I started off in 1977 for a few months under contract.  I had already applied for a position, and when the position opened up, I was hired by the agency in 1978.  I retired in 1995.</p>
<p>Early years and education</p>
<p>
<hi rend="italics">Q:  Okay.  Well let's go back and talk a little bit about where you are from, your education, and anything in that, that suggests why you became interested in the international development business.</hi>
</p>
<p>VREELAND:You will probably understand why I became interested in international affairwhen I tell you that I lived in several countries while I was growing up.  My father was the American Consuin Sevilla, Spain.  I was born in Pittsburgh, Pennsylvania, went back to Sevilla where I got my nickname Nena, and then before I was one year old I left Spain for Canada where my father was assigned as Consul in Montreal.  We lived there for several years, then we moveto Buenos Aires, Argentina.  My parents were divorced, and my stepfather worked for Armour and Company, which had business interests in Argentina, so we remained in Buenos Aires until 1945, then moved to Sao Paulo, Brazil.  We returned to the United States in time for me to go to high school.  I finished high school in New Jersey and went on to the University of Rochester.</p>
<p>
<hi rend="italics">Q:  Where in New Jersey?</hi>
</p>
<pb facs="0002" n="Page0002"/>
<p>VREELAND:Englewood, New Jersey, very near New York City.  My stepfather worked in Manhattan; we lived in Englewood, a very pretty town.  Then I went to the University of Rochester where I got my BA.</p>
<p>
<hi rend="italics">Q:  What did you major in?</hi>
</p>
<p>VREELAND:I majored in political science and economics.  I had started in the sciences but switched to the arts program in my second year.  When I graduated in '55, I came down to Washington to go to graduate school at the School for Advanced International Studies of Johns Hopkins University.  I got my masters degree in 1957 from SAIS.  That, of course, was in international studies.</p>
<p>
<hi rend="italics">Q:  Was there any particular emphasis?</hi>
</p>
<p>VREELAND:At that time, they only offered a master's degree with a regional specialty, and my region was Southeast Asia.  I learned to speak, write and read Bahasa Indonesia in order to qualify for my degree.  In fact, had the opportunity to welcome President Sukarno of Indonesia in Bahasa when he visited the United States, at a reception at the Embassy.</p>
<p>Q:  How did you find learning that language?  Was it very difficult?</p>
<p>VREELAND: No.  It's written in Roman script, and I enjoyed learninnew languages.  Also, Bahasa has a lot of European words in its vocubulary, and since I already had some Spanish, Portuguese, French and German, it was not difficult to learn.  Somewhat like Japanese is not difficult to learn.</p>
<p>
<hi rend="italics">Q:  Any subject area emphasis in your regional studies?</hi>
</p>
<p>VREELAND:No, it was a comprehensive curriculum.  If anything, the school emphasized economics, international economics, international trade, economic development —  at that time those were areas of particular interest in the international field.</p>
<p>
<hi rend="italics">Q:  You graduated in what year?</hi>
</p>
<p>VREELAND:1957.  It was a two year masters program.</p>
<p>
<hi rend="italics">Q:  Were there any particular professors that stood out?</hi>
</p>
<pb facs="0003" n="Page0003"/>
<p>VREELAND:No —  they were all different.  SAIS tended to attract people, at least a portion of the faculty, from outgoing administrations, so the people we had as professors were often very much on top of current policy and practice in the government in the international field, and that was very good.  It made quite a difference in the quality of the teaching and the interaction and discussions in class.</p>
<p>
<hi rend="italics">Q:  So, what happened after you graduated?</hi>
</p>
<p>Joined the American University Handbook Program - 1957</p>
<p>VREELAND:I graduated a few months earlier than most of my classmates because I had intended — and the school had intended —  that I go to Burma to continue studies toward a doctorate.  That was cancelled at the very last minute because of political conditions in Burma.  The school eventually did close down its satellite school in Rangoon.  So, there I was,  graduated early, my plans changed.  Well, it just so happened that at The American University a program had just started to write handbooks on foreign countries for the Department of the Army.  AU had sent notices around to various universities in the area announcing some positions.  I hurried up Massachusets Avenue with a friend of mind, also from SAIS, also in the same boat, and we applied and were hired.  I started working right away as a researcher writer, drafting chapters for these handbooks.  The handbooks were very comprehensive; they covered everything about a society, its social organization, its political dynamics, its economic structure, culture, military organization, religion — with chapters devoted to those specific topics.  Initially, I handled mostly the chapters on economic conditions.</p>
<p>
<hi rend="italics">Q:  Why was the Army interested in all this depth in all of these countries?</hi>
</p>
<p>VREELAND:Because of the military assistance program which started after the Cold War began, the US Army found itself in a role that was somewhat different from the one it had been accustomed to during the Second World War and even the Korean War.  Also the Army was engaged more at that point in what they called psychological warfare, which required a good understanding of a country's overall social strengths, weaknesses and values.  At any rate, the Army needed officers who haa fuller understanding of the social conditions of the countries with which they were dealing, on both sides of the Cold War.  These books were written for that purpose initially; however, when they began being published, they became quite popular.  Businessmen, people in the State Department who were traveling overseas also found them to be very helpful, so they became minor best sellers at the Government Printing Office..</p>
<p>
<pb facs="0004" n="Page0004"/>
<hi rend="italics">Q:  Were you free to write what you found, or was there any ideological effort to alter the text or write about that or have that point of view?</hi>
</p>
<p>VREELAND:No, we had to be as honest as possible or otherwise the handbooks would be useless to the client.  The one caveat was that if we had to use classified information, the books were classified.  Every effort was made not to use classified information if we could find other sources because we wanted to have the books as widely available as possible and easy for the client to use..</p>
<p>
<hi rend="italics">Q:  What were your main sources?</hi>
</p>
<p>VREELAND:Many — newspapers, memoirs, standard works.  There was a lot of interviewing of people who had been in the countries, both nationals and visitors.  It was very wide ranging research.</p>
<p>
<hi rend="italics">Q:  Which countries did you work on?</hi>
</p>
<p>VREELAND:I worked on handbooks on 17 countries; about half of them were in Asia and the Pacific because that had been my region of interest in graduate school.  I did work on 3 African countries, several in the Middle East and one in South America.</p>
<p>
<hi rend="italics">Q: Did you find that people found them quite useful and there was quite a demand?</hi>
</p>
<p>VREELAND:Yes, there was.  And the word we got was that they were quite useful.</p>
<p>
<hi rend="italics">Q:  Are they keeping this up?</hi>
</p>
<p>VREELAND:I understand the program has continued.  It is no longer housed at The American University, but I believe at a division of the Library of Congress.  The books themselves have gotten much shorter. When I was with the program, they were really very comprehensive and thorough, quite long books.</p>
<p>
<hi rend="italics">Q:  Did they give a true picture of the world?</hi>
</p>
<pb facs="0005" n="Page0005"/>
<p>VREELAND:Well, at least when I left the program, the world that was considered of concern to the US Army during the Cold War.  What was called the Soviet Bloc at that time and the third world were the main areas of contention, so the handbook program concentrated on those countries..</p>
<p>
<hi rend="italics">Q:  The Army decided which countries.</hi>
</p>
<p>VREELAND:Yes, the client decided not only which countries would be covered but how often the books should be updated and revised.</p>
<p>
<hi rend="italics">Q: Did you enjoy that work?</hi>
</p>
<p>VREELAND:Very much.  We would spend maybe six months on a book, and we became something of experts on that country.  It was such an intensive research process, you really immersed yourself in knowledge about that country.  You never actually visited the countries.  That was a disappointment, but it wasn't a handicap in getting the information we needed.  We often had advisors who had been to the countries.</p>
<p>
<hi rend="italics">Q:  How big a staff was it?</hi>
</p>
<p>VREELAND:At that time, there were abousix teams operating, each had 5 or 6 people.  Each team was multi-disciplinary.  There were military specialists, economists, political scientists, anthropologists, other social scientists.  Each team member was assigned several chapters to research and write.</p>
<p>
<hi rend="italics">Q:  How long did you do this?</hi>
</p>
<p>VREELAND:The other interesting thing about my work there was that the person who hired me, later married me.  My husband was research director at this program when I applied for the job.  We were married a couple of years later.</p>
<p>
<hi rend="italics">Q:  He was in the program.</hi>
</p>
<pb facs="0006" n="Page0006"/>
<p>VREELAND:Yes, he was heading up the research side of the program, and had been responsible for preparing the overall scope and research guide for the handbook chapters.  Another great thing about the job was that the people who worked there were very interesting.  This was a program that gave a lot of people, young people especially, a chance to get their feet wet after graduate school and be in Washington and move on after a time with the handbook program.  For example,  Allison Herrick went on to AID.  Others went on to jobs in other government departments and to academia.</p>
<p>
<hi rend="italics">Q:  How many years were you there?</hi>
</p>
<p>VREELAND:I was there for one six year stint after graduate school, and then I returned just before I went to work for AID and worked there for an additional four years.  In the interim I worked for...</p>
<p>
<hi rend="italics">Q:  The interim being what years?</hi>
</p>
<p>VREELAND:Let's see, I left the handbook program in 1963.  I went to work for what would now be called a beltway bandit.  At that time it was called a think tank.  It was an organization that did contract work for the Department of the Army, much of it in weapons systems.  I worked there for about 8 years, and then left there to stay at home for a couple of years while my daughter was in middle school.  We had also moved to a new community in suburban Maryland, and so between the move and my daughter's changing schools, it was time to take a break.  I did work briefly for a group nicknamed “Nader's Raiders” on a project that consumer advocate Ralph Nader had going on at the time, and then in 1973 I went back to the handbook program at the American University for another four years.</p>
<p>
<hi rend="italics">Q: About 10 years all together.</hi>
</p>
<p>VREELAND:Yes, about 10 years all together in the handbook program.  Then in 1977 I left the handbook program and went to work for AID.</p>
<p>Q:  Why did you leave?</p>
<p>Helped the initiation of AID's Development of Information Service - 1979</p>
<pb facs="0007" n="Page0007"/>
<p>VREELAND:The handbook program was cut way back, and some of the research teams were disbanded.  At that time I was what they called a team chairman, a senior position, so I was given the choice of staying on as a researcher writer, which I had been initially, or else leaving the program.  I chose to leave the program, thinking it would be a good time to make a switch if I were going to change jobs, because I was in my forties, so that is what I did.  It was an interesting experience because for about three months I collected unemployment insurance.  That was, Haven, a radicalizing experience. I applied for a position at AID. Allison Herrick was the one who told me about it.  She said they were starting an information program in the policy bureau, now the Bureau for Policy and Program Coordination, or PPC.  She said it sounded right up my alley because AID wanted somebody to manage the service part of the program, handling the requests for informatioabout AID's experience with various projects and programs, analyzing and organizing the information and packaging it in ways that would be useful to the clients.  So, I went in and talked to Maury Brown.</p>
<p>
<hi rend="italics">Q:  What was his position at the time?</hi>
</p>
<p>VREELAND:He was heading up the development information program.</p>
<p>
<hi rend="italics">Q:  Was this a new program at the time?</hi>
</p>
<p>VREELAND:It was.  Maury had worked in other parts of AID before, I think in IRM.  The new development information system was located in PPC, together with a small library of program documents located on the ground floor of the State Department building.  In addition to being interviewed by Maury I waalso interviewed by the contractor who was responsible for settinup the whole plan and procedures for the information system and for whom I would be working in the interim.  The person that I would be working for under that contract was Molly Hageboeck, so I was interviewed by her.</p>
<p>
<hi rend="italics">Q: She was with a private firm.  What was the firm's name?</hi>
</p>
<p>VREELAND:Practical Concepts Inc.  Later several people left that firm and formed another company called Management Systems International.  At any rate, I worked for Practical Concepts under that contract for several months and then thPPC job finally came through.  I managed the information analysis part of the program for about a year.</p>
<p>
<hi rend="italics">Q:  What was the job?</hi>
</p>
<pb facs="0008" n="Page0008"/>
<p>VREELAND:This was a new system and a new service for AID staff, and part of the job was to get people aware of it. The main part of the job was to help AID folks frame the kinds of questions that woulaccess information about the agency's program experience, and then to provide them the information in ways they would find useful.  Based on the information we provided, the client could then order complete copies of documents or portions of documents.</p>
<p>
<hi rend="italics">Q:  Was the information there?</hi>
</p>
<p>VREELAND:The information system was computerized.  It was an automated system in which documentation, especially project papers, project and program evaluations and other evaluative- type information was summarized and entered into a database and key-worded in various ways so that it could be accessed.  There was a thesaurus of key words that we used, but the system was evolving all the time.  By the time I got there, the system had reached a point where it was important to interface closely with the clients so that their feedback could have an influence on the further development of the system, making it increasingly responsive to the clients. It was fun.  It was fun to be on the ground floor of a new system when it was just starting up.</p>
<p>Q:  It was based in PPC at that time.</p>
<p>VREELAND:The development information office was organizationally located in PPC, but we were physically located in Rosslyn.</p>
<p>
<hi rend="italics">Q:  Was it linked to evaluation at all?</hi>
</p>
<p>VREELAND:Oh, yes, because much of the documentation in our database was evaluative information — evaluation reports — and also because of our organizational link to PPC.  The evaluation office was also in PPC so there was a fair amount of interaction that went on both administratively in the sense that Maury went to the meetings with the other folks in the bureau, and substantively.  The packages of information we put together in response to clients' questions were based primarily on the documentation in our database, but we also checked with bureau people to make sure that what we put together would be useful and still relevant under current policy, so there was a fair amount of interaction.  Of course, another matter that needed close communication with the evaluation office was making sure that all the agency's evaluation reports were acquired by the information system and entered into our database.  That was a constant problem.</p>
<p>
<hi rend="italics">Q:  How many were working on this project?</hi>
</p>
<pb facs="0009" n="Page0009"/>
<p>VREELAND:I had a staff of about five analysts.  That was my job for the first year and a half with the agency, and then there was a reorganization.</p>
<p>
<hi rend="italics">Q:  Before we go on, what was the demand for this, what was the interest?</hi>
</p>
<p>VREELAND:Well, there were mixed reviews at the beginning.  The information system turned out raw information.  There were summaries of project papers and evaluation reports, for example.  These reports had been key-worded by people who were not all that familiar with the agency.  From my perspective the information really needed a lot more analysis and synthesis and thought before it could be useful to a busy client in the field.  The complaints we got initially were, “You are sending us raw print-outs of summaries;  not a help to us;  too much information; we need something different”.  Essentially that was the kind of reaction we got at first, although some people were absolutely delighted to have such rapid access to project documentation and evaluations of the agency's experience and so-called “lessons learned.”  I think the majority found the responses we prepared interesting but too long with too much information to wade through it and thus not helpful.  I think it's a familiar problem, and one that the information system, which as you know has continued over the years and still exists, had to come to grips with, and I think in the end they did so very successfully.</p>
<p>
<hi rend="italics">Q:  Can you tell me what people were most interested in, what questions they asked?</hi>
</p>
<p>VREELAND:No — it's hard to recall specific questions, they were so varied.  I remember that one of our more successful products was what were called topical packages. These were self initiated.  They weren't in response to a specific query, buwere rather in response to what we saw as current topics that were likely to come up again and again in new project design, in the planning process of the agency or new areas that the agency was getting into where there was not a lot of available information about the agency's own previous experience. We would put together a package of information in a simplified, point by point way and send it out or announce it in various ways.  Those tended to be fairly popular.</p>
<p>
<hi rend="italics">Q:  Do you remember any specific ones?</hi>
</p>
<p>VREELAND:  I remember we did one on the environment.  This was when people were just beginning to get into some of these areas.</p>
<p>
<hi rend="italics">Q:  This was project information.</hi>
</p>
<pb facs="0010" n="Page0010"/>
<p>VREELAND:Yes, we would draw on other sources, other organizations that had been involved in these kinds of activities.  I remember we did one on irrigation projects. There was a heavy demand for more information about some of the problems associated with irrigation projects, as the agency moved from large-scale to smaller scale irrigation works. We could send these topical reports to the technical people,  saying, “Here is something we put together.  We know there is a lot of planning going on in this area.  We thought you might find this information useful.”  We could identify from the early planning documents which bureaus and which missions were going to be involved in designing the projects, so we could target the reports.</p>
<p>
<hi rend="italics">Q:  Technical people were interested in this or were they saying who are you?</hi>
</p>
<p>VREELAND:Project design people, and people in the program offices in missions.From the technical people, there was a certain amount of “who are you?”, but the folks who set up the procedures for the information system were pretty savvy.  Our instructions were and, I believe,  continued to be to check with the central bureau technical people; these people were themselves often the very specialists who went out to the field on project design teams.  The other important thing was that we didn't pretend to be the final technical word on the subject.  Our interest was in what happened in particular kinds of projects that made them work or not work well, based on the evaluation information we had or could find.  Often, it wasn't a technical problem that had affected the results of a project; it might be a social issue; or a management issue.  In that case we felt on much firmer ground.  It was unnecessary to check everything out with the technical folks, but we always kept the line open to them.</p>
<p>
<hi rend="italics">Q:  Anything else about that particular period?</hi>
</p>
<p>VREELAND:No —  well, yes,  there was.  When I was looking over my records to prepare myself for this interview,  I recalled how significant it seemed at the time.  There was a reorganization, and the development information system was taken out of PPC.</p>
<p>
<hi rend="italics">Q:  This was in what year?</hi>
</p>
<p>VREELAND:I think it was called the reorganization of 1977, but it actually took place aroun1978 1979.  At any rate, the development information office was removed from the policy bureau and placed in the central technical bureau, and my analysis unit was placed under the technical bureau librarian.</p>
<p>
<hi rend="italics">Q:  Do you understand the reasoning why that was done?</hi>
</p>
<pb facs="0011" n="Page0011"/>
<p>VREELAND:I never did fully understand.  I believe the reasoning was that the agency didn't need two separate information sources or libraries, one in the technical bureau focussed on technical information, the other in the policy bureau focussed on development experience.  It makes sense; they both involved disseminating information to agency staff, and both were in central bureaus.</p>
<p>
<hi rend="italics">Q: Was there concern that being in the policy bureau the information might be tuned to the policy of that time?</hi>
</p>
<p>VREELAND:No, I don't think that was an issue.  I don't remember hearing that, although I think there was always some understandable tension between the technical folks and the policy folks.  In any case I found myself operating in a very different philosophical environment.</p>
<p>
<hi rend="italics">Q:  You were put in the library?</hi>
</p>
<p>VREELAND:Yes.  We were still physically located in Rosslyn; my small analysis unit then became organizationally part of the library, which was itself made part of a now enlarged office.</p>
<p>
<hi rend="italics">Q:  The library was where?</hi>
</p>
<p>VREELAND:It was also located in Rosslyn, in a neighboring building.  At any rate, what I soon came to understand was that the librarian, my immediate supervisor, was not a believer in the usefulness of evaluative information.  He told me that he firmly believed that every case of a development project was unique and idiosyncratic, and that there was really very little to be learned from past experience that would be applicable to a current situation or future planning.</p>
<p>
<hi rend="italics">Q:  What was your view?</hi>
</p>
<p>VREELAND:My view was you can learn a lot from experience and you can share the knowledge.  It might not all be relevant, but much of it will be.  At some point the knowledge ceases to be relevant, but at some later point it may become relevant again.  At any rate, it was at that point that I realized that my role in the new organization was not going to be very interesting or important, and I actively sought a shift, a move within the agency.  As I said, we had worked closely with the evaluation office before the reorganization, and there was an opening in that office, and I applied, and I got it.  Essentially, I had moved out of PPC briefly, and then I moved back again.</p>
<p>Joined the AID Evaluation Office - 1979</p>
<pb facs="0012" n="Page0012"/>
<p>
<hi rend="italics">Q:  That was in 1979.</hi>
</p>
<p>VREELAND:Late 1979.  I stayed in the evaluation office until I retired from AID.</p>
<p>
<hi rend="italics">Q:  What was the evaluation office situation at that time?  What was the state of evaluation?</hi>
</p>
<p>VREELAND:When I arrived at the evaluation office in 1979, it was about the time that Bennet assumed the administratorship of the agency.  There was a lot of excitement on the evaluation side.  The evaluation office was headed by a man named Robert Berg, Bob Berg, who really wanted to put evaluation on the map and felt very strongly there had to be within the policy bureau a central unit with an independent evaluation function.  Up until that time, evaluation in the agency had been almost entirely decentralized — that is, each unit in the agency, each bureau, each mission, undertook its own project evaluations.There were some general guidelines, but evaluation was by and large run independently by the agency's operating units.</p>
<p>A number of things were also going on around that time.  There was a lot of discussion about the “failure” of the so-called basic human needs or New Directions thrust of the agency's foreign aid program which had started around 1972 or 1973, before I joined the agency. There was a lot of criticism starting about how successful that thrust had been.  In retrospect, I doubt whether the agency really gave the New Directions approach much of a chance to be tested out.  In any case, that criticism culminated in the publication by the World Bank of a report by Elliot Berg, I think it was in 1980 — a report on Africa which made a strong plea for the dismantling of command economies and promoting the role of the private sector and free market economies.  So, in terms of the theory of development, there was a lot of discussion going on at that time, as well as discussion about the role of evaluation in the agency.</p>
<pb facs="0013" n="Page0013"/>
<p>Bennet comes in to head up the agency, and Bob Berg and Dick Blue (who was also in the evaluation office at that time) I think took the opportunity to make a plea for a strong central evaluation function, and Bennet agreed.  I'm not sure exactly who began the process of rethinking the role of evaluation — either Berg and Blue went to Bennet, or else as often happens at the beginning of a new administration, Bennet came in and said in effect: “We are going to have stronger accountability, or evaluation”  — he didn't use the word evaluation necessarily — “We are going to take a hard look at what we are accomplishing, if anything, with all this foreign aid.  I want to know what we have accomnplished.  What impact have we had?”  In any event, Administrator Bennet, Berg, and Blue got their heads together and the office of evaluation became a much more important part of the policy bureau and of the agency.  Under Bennet's leadership, that office began a series of impact evaluations.  These were centrally run, they were very high profile.  The teams were pulled together from throughout the agency and were sent to look at projects; when they came back, they debriefed Bennet directly.  A meeting was set up with him, and the team went into his office, and they told him face to face what they found.  So, it was a tremendously high profile effort.  I was asked to do two things as part of that effort.One was to be the senior editor of the publication of the new impact evaluations, the publication series.  I also headed up what was called the Administrator's Task Force on Evaluation which was to look at the future role of evaluation in the agency and how it would be organized.  That was the first of several such efforts I experienced over my subsequent years in the agency.</p>
<p>
<hi rend="italics">Q:  What conclusions did you come to?  What were you recommending in that first study?</hi>
</p>
<p>VREELAND:  In that first study, it was really to make a decision about sharing the responsibility for evaluation among different parts of the agency.  Who would be responsible for what?  What kind of resources would be available?  Essentially the study affirmed the need for a central evaluation function while continuing to permit other units of the agency to carry out evaluative work as they saw necessary for the implementation of their programs.The study placed considerable emphasis on having each operating bureau be responsible for preparing an annual plan and budget for evaluation work, and for reporting on the extent to which the plans were carried out.  There were other suggestions associated witsharing experience, lessons learned. Those were the elements I remember most clearly.</p>
<p>
<hi rend="italics">Q:  What were some of the issues?</hi>
</p>
<pb facs="0014" n="Page0014"/>
<p>VREELAND:Well, there was a lot of contention.  Not really much contention about lessons learned.  It was clear that the information system had to come to grips with how to get the information out in more useful ways.  But there was considerable argument over who would look at the foreign aid program, who would evaluate the program?  Would it be the responsibility of the geographic bureaus, the central technical bureau, or the policy bureau?  I'm not sure that anybody was fully satisfied, and I was given a meritorious honor award for having successfully extracted some degree of agreement from that task force, includinagreement that there would continue to be a central office with responsibility for impact evaluations..</p>
<p>
<hi rend="italics">Q:  Was there resistance to that?</hi>
</p>
<p>VREELAND:Oh, yes, there was. Everybody liked to have their own evaluation function because they could make it say what they wanted it to say.  PPC, the central policy bureau, was never particularly popular in the agency.  People felt that if the evaluation function was housed solely in the policy bureau, that bureau would have yet another stick over the geographic bureaus, a potential weapon.</p>
<p>
<hi rend="italics">Q:  What was the view of the quality of evaluations at that time by the regional missions?</hi>
</p>
<p>VREELAND:Very mixed.  There was little or no attention in the mission-sponsored evaluations or the bureau-sponsored evaluations given to the impact of individual projects, much less a group of several projects in a country.  This is what Bennet was most concerned about.  He felt that we as an agency really had to have a better grasp of what impact we were having so that we could tell the foreign aid story convincingly to the Congress and to the American public.</p>
<p>
<hi rend="italics">Q:  That was his principal motivation?</hi>
</p>
<p>VREELAND:I believe so. I don't think it was entirely a matter of missions' and bureaus' deliberately avoiding looking at the impact of their projects.  The fact is that impact is very difficult to assess, and the usual methodologies for evaluating impact were difficult if not impossible to apply in the case ointernational development programs in third world countries.  It is a very difficult set of observations to make about a project, what impact does it have.  In any case the bureaus did not focus on impact; the missions did not focus on it in the evaluations they did.</p>
<p>
<hi rend="italics">Q:  What were they focusing on?</hi>
</p>
<pb facs="0015" n="Page0015"/>
<p>VREELAND:Mostly implementation issues, input output questions, management and administrative problems, contractual problems,  that kind of thing.  These were important and practical issues, but they didn't tell you much about project results. One of the arguments or defenses against centralizing the program of impact evaluations was that it was difficult to measure impact.  Most of these programs and projects had not been in effect long enough to assess their full impact.  One of the benefits of the Bennet-sponsored series of impact evaluations was that it demonstrated that you could get a sense of the impact of a project within some reasonable period of time after its completion.  That was an important step in the agency's approach to evaluation, I think, to learn that you could actually do that.  Moreover, the one impact evaluation that was undertaken by mistake, in that the project had not yet ended, turned out to be exceptionally useful because there was still time to reorder resources within the project before the project terminated.  The evaluation suggested a number of steps that could be taken to improve the probability of its intended impact.  That was alsan eye-opener for people.  In a sense, we were learning how to focus on issues closely related to impact, how to do that well.</p>
<p>
<hi rend="italics">Q:  How were these impact studies carried out?</hi>
</p>
<p>VREELAND:They were carried out under Dick Blue in the studies division of the evaluation office.  As I said, the teams that carried out the evaluations were pulled together from throughout the agency.  They were made up of agency direct-hire staff, not outsiders.</p>
<p>
<hi rend="italics">Q:  Fairly senior?</hi>
</p>
<p>VREELAND:There was a mix.  The teams rarely included, I don't think any of them included, foreign nationals in the country, which I think was a mistake.  The very successful evaluations, however, were successful in the sense of leading to action, a remedial action of some kind or a follow-up action, a change in policy or an improvement in design, and they did engage the actors who would be important in making that happen, either as they were interviewed, or debriefed, or participated in a follow-up conference.  At some point in the evaluation process, these key actors became involved.  That was important, I think.</p>
<p>
<hi rend="italics">Q:  Give us some examples of some ones you did at that time.</hi>
</p>
<p>VREELAND:I was not involved in directly in the evaluations, only in their later publication.  My job dealt with the admininstrative side of evaluation planning for the agency, and overall evaluation guidance on methods.  I was in a separate division in the evaluation office..</p>
<p>
<hi rend="italics">Q:  We'll come to that, but what was your impression of the evaluation process?</hi>
</p>
<pb facs="0016" n="Page0016"/>
<p>VREELAND:My impression of these impact evaluations was very positive.  People were doubtful at the beginning, as I said.  When the evaluation studies started coming out in published form and started circulating, they became increasingly popular.  People respected the findings, the quality of the work.  I think the evaluation teams had a lot to do with this eventual acceptance — they were topnotch staff and respected by their peers, people with solid social science research experience.  And the methods the teams used became increasingly refined and rigorous.</p>
<p>
<hi rend="italics">Q:  What were the subject areas?</hi>
</p>
<p>VREELAND:Let's see.  They were all across the program.  There was a series on rural roads, a series on health, a series on agricultural growth and development.</p>
<p>
<hi rend="italics">Q:  Was irrigation one of them?</hi>
</p>
<p>VREELAND:Irrigation was one of them, an early one, right.  The practice was to evaluate several projects in a particular program area, like small-scale irrigation or rural health, and then to look for patterns or common lessons.</p>
<p>
<hi rend="italics">Q:  Different projects.</hi>
</p>
<p>VREELAND:Similar projects but in different countries.  Then all the evaluations — maybe five or six in a specific program area — would be pulled together in a more comprehensive summary report usually involving some kind of a conference or seminar where a lot of people were brought in, the experience was reviewed, additional experience was solicited, and a final report was issued.</p>
<p>Q:  How long did the process take for one?</p>
<p>VREELAND:I don't remember, but the teams were not in the field for very long.</p>
<p>
<hi rend="italics">Q:  How could they get a sense of the impact if they were out for so short a period?</hi>
</p>
<p>VREELAND:I don't know what you mean by short a period.  This was maybe four weeks.</p>
<p>
<hi rend="italics">Q:  Well, that's not very long.</hi>
</p>
<pb facs="0017" n="Page0017"/>
<p>VREELAND:Three or four weeks, something like that.  I don't think it was more than a month.  In addition, there was about a week of intensive preparation and some training before a team left, and about a week to finish the report when they returned.</p>
<p>
<hi rend="italics">Q:  Was there a problem in getting key agency people to participate, to take a month off?</hi>
</p>
<p>VREELAND:I'm not aware of a serious problem while Bennet was Administrator, because this was the Administrator's evaluation series, and it became known that you had the opportunity to debrief the Administrator directly.  There may have been some difficulty at the very beginning, but when word got around, I don't think there was a big problem getting people freed for four to six weeks.</p>
<p>
<hi rend="italics">Q:  You say that during this period you were working on the evaluation system for the agency.</hi>
</p>
<p>VREELAND:As I said, I managed the intra-agency task force in 1980.  A large part of my work for a year or so afterwards was to help implement the recommendations of the task force.  For a couple of years I was also the editor in charge of the publication of the impact evaluations,  and set standards and quality controls for the publications so they would have a certain consistency of presentation, style and so forth.  We also began to publish a series on evaluation methods.  That was one part of my job.  The other was to follow through with the recommendations of the task force.  That involved more systematic review of the planning of evaluation work in other parts of the agency and monitoring how well the plans were actually carried out, making sure that the resulting evaluation reports were entered into the information system of the agency, working on evaluation methods when methods issues came up, that kind of thing.</p>
<p>
<hi rend="italics">Q:  What were the major changes as a result of this work in the evaluation system?</hi>
</p>
<p>VREELAND:The planning of evaluation work became much more of a formal concern.</p>
<p>
<hi rend="italics">Q:  What was involved in that?</hi>
</p>
<p>VREELAND:We requested that the bureaus organize their evaluation function, tell us what theimissions' plans were for evaluation work and when evaluations were scheduled, the amounts budgeted for evaluation, when previosuly-scheduled evaluations had been postponed, when we could expect the reports to come in.</p>
<p>
<pb facs="0018" n="Page0018"/>
<hi rend="italics">Q:  Were there certain requirements about when they should evaluate, how often and all that?</hi>
</p>
<p>VREELAND:There were some requirements.  Missions were expected to continue doing routine evaluation work during implementation.  More in-depth evaluations, especially for major projects, were to be done about halfway through the project's life and then again when the project was completed.  The requirements were rather mechanical, but at least they gave a basis for making sure that the major work of the agency, the major projects would be looked at periodically through an evaluative lens or perspective.  There were some suggestions about using external evaluators, people not associated with the project.  I recall they were fairly basic, minimal kinds of requirements.</p>
<p>
<hi rend="italics">Q:  Did you find the bureaus or the missions were responsive to this or not?</hi>
</p>
<p>VREELAND:They didn't like it.  It constituted another reporting burden on missions, and evaluation had never been popular in the agency anyway.  Over the years of being involved in program evaluation in AID, I eventually came to my own conclusions about why that was the case.  Fundamentally the agency operated on some basic contradictions that were never resolved and probably never will be and really don't have to be resolved for the agency to stay in business.  One is that you could run a development program with the same money that you were using for political purposes, for short term foreign policy purposes.  Foreign aid was, after all, primarily a tool in our Cold War toolkit, mainly to win friends and influence people.  So development objectives were of much lower priority.  The circumstances or environment created by that internal contradiction over the years was such that agency staff didn't really want to evaluate the program in terms of their purported development objectives.  Evaluators became anathema. “ Here come the evaluators” was like “Here comes the inspector general” or “Here come the auditors.”  So evaluation was something you tried to put off.  Now you can imagine that if our own staff were resistant, then the host country's staff would have been at least doubly so.  They were living in societies where conditions were already difficult enough, in terribly resource-poor countries, and the threat of having foreign assistance resources cut off was even worse, so your counterparts were even less likely to want to look hard at a project.  There was a lot of resistance.</p>
<p>
<hi rend="italics">Q:  What evolved after that period with Bennet and so on?</hi>
</p>
<pb facs="0019" n="Page0019"/>
<p>VREELAND:For me, this was a learning experience in the agency, discovering all this for myself.  I decided I was never going to make evaluation popular, but I might make it somewhat more useful and acceptable.  I want to make clear at the beginning: I was a civil servant; I was not a foreign service officer.  I did get to visit a lot of countries during my years in AID, at last count I think it was about 14 countries, for varying periods ranging from one week to six weeks.  But I was not a foreign service officer.  The people I worked with were.  The people who would be helpful in making evaluation useful to the agency were people who had the experience in the field, who had worked in the field for long periods of time and understood why evaluation was such a pain in the neck, yet who nevertheless felt that it could be made useful.  What we did was start a series of training workshops on evaluation in the field.  Around 1983 we started talking about this; and by 1985 we had a series going.</p>
<p>
<hi rend="italics">Q:  Let's come back to that.  What happened organizationally?  Did evaluation stay in PPC or what?</hi>
</p>
<p>VREELAND:Evaluation stayed in PPC.</p>
<p>
<hi rend="italics">Q:  Were there any changes?</hi>
</p>
<p>VREELAND:Yes there were.  A new administration came in.</p>
<p>
<hi rend="italics">Q:  That was under Administrator McPherson at this point.</hi>
</p>
<p>VREELAND:The Reagan Administration, right, and immediately there was new leadership in all parts of the evaluation office.  Bob Berg left, and Jim Turner, who was the head of the systems division where I worked, had already retired.  Dick Blue stayed for awhile as head of the evaluation office.  There was a change in leadership, but less of a change in functions.  The central evaluation studies continued, the methodology support, the planning support continued.  And there was yet another round of making sure that lessons were learned through evaluation.  I mean, several administrators came and went during my stay in AID, and each one in one form or another reiterated the same idea of “let's find out if we are getting any results for all this expenditure of foreign assistance; we should pay attention to lessons learned; we should be accountable”.  These were the themes that each new administrator voiced in one way or another.</p>
<p>
<hi rend="italics">Q:  Do you think they were getting any pressure from the Hill for this or it was their own idea or part of their efforts to defend the program?  Did you hear much from the Hill at all?</hi>
</p>
<pb facs="0020" n="Page0020"/>
<p>VREELAND:My job did not expose me much to the Hill personally.  I did go to a couple of hearings, on my own time because you weren't allowed to go on office time.  I remember I went to the Hill once with my husband to sit in on a hearing.  In the hallway outside the committee room, we happened to meet Senator Sarbanes from Maryland,  which is where I live.  We got to talking and he asked me where I worked.  I said I work at the Agency for International Development.  He said, “Oh? What do you do there?”  I said my job was to help the agency snatch development from the jaws of politics.  He said, “Good luck!”  That expressed, I think, the kind of pressure the agency was always under.  The Hill was really never satisfied about foreign aid.  It never would be.  It was always a little dubious about foreign aid.  And this was under administrations whose leaders were rather forcefully in favor of foreign aid.  President Reagan was not an enthusiast of foreign aid, so there was probably more pressure from the Hill and from the White House during this time.  However, President Reagan, regardless of other views he may have had about foreign aid, did, I think, acknowledge it as a useful Cold War tool, and once he made a decision to keep it going, he was very clear about it.  He went up to the Hill and said here's what we are going to do, and here's what we need, and the Congress gave it to him.  This was true of foreign aid.  The program did not suffer as much as some people thought it might during his administration. Although it certainly lost a humanitarian or human welfare focus, and stressed private sector growth and free market development.</p>
<p>
<hi rend="italics">Q:  Was there more of a Cold War orientation to what he was after?</hi>
</p>
<p>VREELAND:Oh, yes, I think so. If anything the political aspect of the program, which was always preeminent, was probably even more apparent.  This was the period of the final, big pitched battle with the “evil empire” at the end of the Cold War.</p>
<p>
<hi rend="italics">Q:  I notice you made a statement to Senator Sarbanes that your job was to snatch development out of the jaws of politics.  What did you mean by that?</hi>
</p>
<p>VREELAND:I meant that as an evaluator, I was in a position to push hard on lessons learned in an environment where the program was predominately a political program.  To the extent that the sharing of experience and the whole process of evaluation could support actual development, then that was getting some development out of a predominately political program.  That's what I meant.  By that time, as I said, I was no longer convinced that foreign aid was predominately a development program; it was a predominately political, foreign policy program.  That was okay, but I had to keep that in mind in terms of the expectations I had about my role in evaluation in the agency and the expectations I placed on others about the actual performance and development results of their foreign aid projects.</p>
<p>
<pb facs="0021" n="Page0021"/>
<hi rend="italics">Q:  How did you characterize this political objective?  What did you perceive as being the role of AID?</hi>
</p>
<p>VREELAND:Well, foreign aid was the Secretary of State's walking around money.</p>
<p>
<hi rend="italics">Q:  To do what?</hi>
</p>
<p>VREELAND:  To promote short-term foreign policy objectives that were designed principally by the State Department — as I said, especially to win and keep friends during the Cold War. That was how State saw their job.  I'm not going to take issue with that.  My concern was not to pretend the emperor had some clothes on that in reality he did not really have on.  Most the the time, the agency was able to wriggle through the contradictions, but from time to time I would hear about situations that would be blatant examples of a conflict between short-term foreign policy objectives and longer-term development objectives.</p>
<p>
<hi rend="italics">Q:  Do you remember any of those?</hi>
</p>
<p>VREELAND:  I wouldn't repeat them because they were mostly told to me by others. I did experience one myself which I'm happy to share with you.  I went down to El Salvador...</p>
<p>
<hi rend="italics">Q:  What year was this?</hi>
</p>
<pb facs="0022" n="Page0022"/>
<p>VREELAND:It was in the late 1980s.  I went down there to help the mission put together an evaluation plan for their program.  The capital had just gone through an earthquake, a bad one.  It had been a really tough time, and there was the insurrection or civil war still going on.  As it turned out, my work there required my helping the mission recover an effective mode of operation, get the program back on track and then measure its accomplishments.  The mission was supporting some export promotion projects and some other projects for which it was very important that an anticipated exchange rate reform be implemented.  If it were not, then a sizable chunk of the mission's program would be undermined.  The government claimed — and these were doubtless legitimate claims — that if this reform were implemented, it would increase domestic prices for certain goods, staples, and they could conceivably have a very serious problem on their hands with a public uproar, possibly riotand so forth, at a difficult and politically tense time. The mission naturally insisted that the government take on the reform, despite the political risk that it would be unpopular.  The issue went up between the Mission Director and the Ambassador, and they could not reach agreement.  So the issue was sent up to AID and the State Department in Washington, and the word came back to the mission, “Back off.”No question, this was a short-term foreign policy objective, a legitimate one.  The host country government was worried.  The State Department decided not to push for reforms, not to rock the boat.  It was clear, however, what the implications were for the mission's foreign aid program:  a big chunk of the program was going to be jeopardized, and that was that.  That was an example I personally experienced, but there were other examples othis contradiction all the time.  As I said, I had to be very careful about what expectations I could have about actual development results .</p>
<p>
<hi rend="italics">Q:  How do you build those kinds of factors or did you in AID build those into the evaluation process?  Were these political situations ever mentioned or taken account of?</hi>
</p>
<p>VREELAND:I don't know if those particular projects in El Salvador were ultimately evaluated.I am assuming that if they had been, the evaluators would have at least noted the political factors, because they were so glaring; but perhaps not.  There was no requirement to do so.</p>
<p>Q:  Are you aware of any evaluations that brought out these political dimensions?  You must have read hundreds of them.</p>
<pb facs="0023" n="Page0023"/>
<p>VREELAND:Not many did, and that's why I think there was sort of an avoidance of trying to assess the ultimate impact or results of some of the programs.  Of course, the effects of political factors varied depending on the country and on the project.  But generally I think people backed away from openly discussing the import of some othese factors in a formal evaluation —  there was a tendency to dwell exclusively on the routine implementation or technical issues of which there were often many important ones.  So these evaluations left out some important information about experience that might have been helpful.  I felt that it would have been better if the agency had been clearer about the actual motives of foreign aid in a given country and specified those, and brought in people from the State Department or Treasury or other interested parties as members of the evaluatioteams and made sure that those objectives and motives were also looked at in measuring the effectiveness of foreign aid projects.  For example, were we able to effectively sustain a more stable period in El Salvador and what was accomplished during that period that was of political benefit to that country and to our relations with that country? I wouldn't see simply “not rocking the boat” as necessarily a legitimate US objective.  I would have preferred it if the United States had said that we had other important purposes that we wanted to accomplish in that country at that time, and we needed a period of calm and continued stability, and then looked at whether we did accomplish those purposes or not.  I felt that it would be more useful and certainly more legitimate if the whole range of purposes had been looked at in our evaluations and brought out as part of our experience and lessons learned.</p>
<p>
<hi rend="italics">Q:  Does that mean that in a way the development assistance program is unfairly judged because it is supposed to be looked at only on its own specific purpose and the political context is never brought out.  It may have had a significant impact on whether the program really worked?</hi>
</p>
<p>VREELAND:I think so.  I think that the development program was often unfairly judged, but as I said, much of the time we were smart enough to avoid judging ourselves in terms of development results.  The tendency in evaluation in the agency except for a few of the impact evaluations was to look at other issues.  But, if we were taking development results seriously, as we tried to do in the impact evaluations and other centrally-managed evaluation studies, I think there would be a certain amount of unfairness in the standards we would apply.  More than that, I think it was too bad for the program's overall success.  If we had placed more emphasis on the political aspects during evaluation,  who knows what we could have accomplished?  If we had moved more deliberately into that domain in our evaluations — into the overlap between development and politics and development and foreign policy — we might have opened up other, quite different possibilities for accomplishing important changes. We in the agency didn't fully acknowledge the short-term political motives of foreign aid, except in the very general sense that development was in the interest of the US and was part of our foreign policy, and so we may have missed some very interesting possibilities.</p>
<pb facs="0024" n="Page0024"/>
<p>The reason I say that is one experience I had.  I was asked to go to Senegal twice — by the way,  I loved Dave Shear; talk about memorable personalities; that Mission Director was something else!  At any rate, the reason for one of these trips was to help that mission put together a monitoring and evaluation side of a strategic plan for their program.Shear wanted the planning process of setting goals and priorities for the foreign aid program to be a very collaborative process, and he was very up front about the politics of the situation.  He had no hesitation about dealing immediately with a lot of political issues in preparing a development strategy and involving the counterparts in the process.  Potential problems — problems analogous to the foreign exchange reforms in El Salvador — were looked at, and it was understood that the government's perspective was to be taken into account.</p>
<p>
<hi rend="italics">Q:  Explain that kind of approach a bit more so people understand it.</hi>
</p>
<p>VREELAND:Well, the simplest way I can describe it was the collaborative nature of the planning process that was initiated.  Many of the planning exercises that I have been involved in,  CDSS or strategic planning, long-term planning as well as the design of specific projects, were very unilateral. In some cases, it was as if the missions distrusted the counterparts with whom the missions would subsequently have to work.  In many cases this distrust was well-grounded.  These were usually corrupt governments; there was an enormous amount of nepotism and favoritism that went on, you name it.  But, that was the real world in which our missions had to operate.  Some directors dealt in that world more effectively than others.  Some directors and officers really wanted to keep most of the host country's institutions and individual counterparts at arm's length from the plans that the missions made and that would be subsequently funded.  I mean, even later when AID started what were called “policy dialogues”, we unilaterally chose the policies about which there would be a “dialogue”.  Other directors would engage some counterparts and keep others away.  Still other Mission Directors would say in effect: “Look, this is nature of the ball game, and we are going to play ball with them all the way because otherwise we won't get anything that is sustainable; we'll get no commitment, nothing lasting.”And they actively sought out the “movers and shakers” in the country, and not just the political leaders but other leaders and opinion shapers — religious leaders, social leaders — to talk about change and values.  My personal observation is that the third point of view is the one that probably was the most effective.Certainly the most collaborative.</p>
<p>
<hi rend="italics">Q:  Was there collaboration in the Senegal case with the Senegalese government officials?</hi>
</p>
<pb facs="0025" n="Page0025"/>
<p>VREELAND:There was a lot of collaboration.  Everything was discussed; there were constant meetings.  Mr. Shear was the only Mission Director I knewho actually told his staff that he didn't want to see them in the office half of every month.  He said in so many words: “I don'want to walk down the hall and see you sitting behind your desks.  I want you out there; I want you talking to your counterparts; I want you at the project sites; I want you out of here.”  I've talked to AID folks who said “Wow, that's really rare.  My Director wouldn't let me out the door because he needed me if something hit the fan.  He wanted to be able to put his hand on the phone and call me into his office right away.”  So, there are styles of management and I think that as far as the state of the art of management goes anywhere in the world, a collaborative style is considered the most effective in most cases..</p>
<p>Q:  Good!  Let's go back to the agency.  I think it was around 1983 that it was reorganized again.  Do you remember that?</p>
<p>VREELAND:You came on board.</p>
<p>
<hi rend="italics">Q:  What happened then? Let's pick up around 1983.  You were in PPC at that time and was there something about a GAO audit of AID's evaluation program.</hi>
</p>
<p>VREELAND:I don't know if it was before or after you, Haven North, came in.</p>
<p>
<hi rend="italics">Q:  It was well before.</hi>
</p>
<p>VREELAND:You came in 1984.</p>
<p>
<hi rend="italics">Q:  Well the fall of '83, but this report had been prepared before that.</hi>
</p>
<pb facs="0026" n="Page0026"/>
<p>VREELAND:Yes.  I think I mentioned earlier that the information system, which was calleDevelopment Information and Utilization (DIU), at that time had been placed in the central technical bureau, and so-called “technical” information had come to be considered much more important than the experiential or evaluative information contained in the agency's program documentation.  The use of evaluation, the use of experience, the extraction of lessons learned from the documentation of the agency sort of fell by the wayside, even though a lot of this information continued to be entered into the DIU database.  There was more emphasis on disseminating mostly technical information.  Then in 1982 the General Accounting Office published a review that focused on the Agency's use — or rather the lack thereof — of information about its experience in designing new programs and projects.  I remember that we in the evaluation office were quite concerned because the development information system was such a potentially valuable tool in getting information out about experience and it was not doing that as well as it could — it could be a very important tool in disseminating information that emerged from evaluations and in turn it could pose other of questions that could be turned back to evaluators to guide their research. We saw thakind of interaction as being important, and it wasn't occuring.  At any rate, we in the evaluation office were required to submit information to the GAO auditor.  I remember that Molly Hageboeck, who had been appointed under the new administration to head up the systems division of the evaluation office, and I saw this as an opportunity to restore the marriage between evaluation and information dissemination in the agency.  I confess we really jawboned the auditor, making the case that it would be terribly important for the information system, if it were to be revitalized, to be housed once again in the policy bureau.  Eventually, that is what happened.  The title of the GAO report itself said something about using the agency's experience [Experience — A Potential Tool for Improving U.S. Assistance Abroad, GAO/ID-82-36, June 15, 1982].  I don't have a copy of that report anymore.  I kept it for a long time because it was such a great summary of the arguments in favor of a close relationship between the evaluation function, policy formulatioand information dissemination in the agency.  It gave the justification for the eventual move of the information system back into the policy bureau.</p>
<p>
<hi rend="italics">Q:  So by 1984 the process had begun, what happened to you?  Where were you? What was your role?</hi>
</p>
<p>VREELAND:My role remained very much the same, focussed on the evaluation system of the agency and evaluation methods.  I'm not sure exactly when the Center for Development Information and Evaluation was formed.</p>
<p>Promoting evaluation in the new CDIE - 1984</p>
<p>
<hi rend="italics">Q:  1984.</hi>
</p>
<pb facs="0027" n="Page0027"/>
<p>VREELAND:You, Haven North, came in to head that up and was the person responsible for finally getting the whole system in place and for getting agency agreement that CDIE should be created as a center and that information was crucial to the functioning of the agency.  That year, 1984, I drafted an action memo for the Deputy Administrator that recommended further strengthening of the evaluation system, and that, if I recall correctly, gave further impetus to the role of the new Center.  Also in that same period in the mid-80's I managed the preparation of a report on the evaluation procedures and practices being used by various member countries in the OECD Development Assistance Committee.  Eventual publication of that report was a fairly important step for the DAC and it also helped reaffirm the strong role of the US in DAC evaluation efforts, a role that had begun earlier when Bob Berg helped establish the DAC's expert group on evaluation.  That was the beginning of a fairly long association I had off and on with the DAC expert group.Looking back on that period, I think there was a shift in the agency's own view, or philosophy, of evaluation, a shift that was very compatible with the stronger evaluation system.  The shift was toward a more explicit and forceful acknowledgement of the purpose of evaluation to support AID and host country development managers, across all levels of program and project management. And it was felt that evaluation should inform management decision-making, and lead to action.  The dissemination of “lessons learned” continued to be important, and there was a much stronger effort to make sure that such lessons had been incorporated into what were known as Action Plans, that is, the missions' forward program planning;  but the needs of other areas of management including host country management were increasingly considered as well.  In turn, this shift influenced our evaluation system and methods.  For example, collaborative evaluation with counterparts was promoted, as were so-called rapid, low-cost evaluation methods that managers could use to get information quickly on both implementation problems and the expected direction of impact or results of their projects.  This management emphasis was certainly evident in two other tasks I had during this period.  These were the design of a workshop on evaluation for training both agency staff and counterparts, and a major revision of the official agency evaluation handbook.</p>
<pb facs="0028" n="Page0028"/>
<p>I mentioned earlier that I had decided that one possible way to get evaluation more widely accepted was through a series of workshops that would give staff hands-on experience in using evaluation as a management tool, so that evaluation would be seen more as a useful exercise than a hindrance to project management.  So we had a week-long workshop designed and pilot-tested, and then the Asia/Near East Bureau sponsored the first regional workshop in Tunis in late 1985.By the way, one aspect of the history of evaluation practice in the agency is the shift in emphasis that took place from regional bureau to regional bureau.  When I joined the agency in 1978, the LAC Bureau was the lead bureau in terms of its evaluation work; by 1984, the reputation for evaluation strength had shifted to the Asia Bureau; and by 1988 the Africa Bureau was clearly in the forefront of evaluation efforts.  I believe the reason for this was the commitment of specific individuals in those bureaus at the time.  Except for the period when it was linked to the Asia Bureau, the Near East Bureau never achieved similar recognition, perhaps because its program was the most highly politicized.  At any rate, the first workshop was held in Tunis, which almost turned out to be a disaster, because halfway through the week, Israel decided the bomb the PLO headquarters in Tunis, but our team recovered and we completed a decent training course.  In 1986, the LAC Bureau sponsored the workshop for its staff, and then the Africa Bureau sponsored two. These “Collaborative Evaluation Workshops” were quite successful.  Although CDIE had designed the workshop, each regional bureau and several missions paid to hold the workshops for field staff, so there was a lot of commitment to making them a success.  And they were — they had rave reviews by and large.</p>
<p>
<hi rend="italics">Q:  You were the primary author of these.</hi>
</p>
<p>VREELAND:Right.  Well, let's say I managed the contract under which the workshop was designed, and had a lot to say about the content.</p>
<p>
<hi rend="italics">Q:  What were they about?  What did they encompass?</hi>
</p>
<p>VREELAND:They were week-long workshops.  They took the participants through some evaluation “basics” — all the way from formulating the issues or questions for an evaluation relating to the purposes and goals of the mission's projects, through creating an evaluation plan, collecting the data, assigning roles and responsibilities, analyzing findings, reporting the results.  The missions would get together a group of people to attend, all of whom had some commitment to the program on the mission side and the host country side.</p>
<p>
<hi rend="italics">Q:  The host country staff was represented as well.</hi>
</p>
<pb facs="0029" n="Page0029"/>
<p>VREELAND:Right, and in countries where there was little understanding of English, the workshops were translated into French or Spanish, simultaneous translation was made available.  So we took the participants through an evaluation process, the elements of evaluation.  They actually conducted a mini-evaluation on the spot.  We took them through types of methodologies.  The most interesting part of the exercise, one that I think had further effects down the line, was the process the workshop went through to clarify the objectives of the mission's foreign aid program anprojects, specifying these more clearly for subsequent evaluation.  That process alone was an eye-opener for many of the participants because they came to see and understand that there were a number of objectives that different people had and that an important part of the development process in each country was to sit down and reach agreement on objectives.</p>
<p>Now in 1985 , you,  Haven, reminded me that the evaluation handbook of the agency was out of date and had not been revised since 1974.  I had a revision of the handbook on my work plan and time kept going by as I worked on the DAC report and workshops and an evaluation I led for the Egypt mission, and you kept asking me where the revised handbook was.  I postponed tackling the job partly because writing an official handbook is a pain in the neck, and in part because I wanted to milk as much experience as possible out of the workshops before I sat down to actually revise the handbook — and it really required a complete re-writing, not simply a revision.  At some point I realized that I just had to bite the bullet and get a new handbook out.  During 1985 and part of 1986, I wrote a very long handbook, and it was too long, too philosophical, too focussed on alternative methodologies.  What was needed was a short “one, two, three, here's what you have to do” kind of handbook.  So I asked a contractor, Chris Hermann, who had done some interesting work for us on model missions, missions that seemed to be doing some particularly useful evaluation work, to take my lengthy tome and pull out of it just a few key elements.  I listed the stuff I thought a one, two, three kind of handbook ought to have in it.  He did that for me, and I took his draft and massaged it some more, and in the end we managed to produce a handbook of about 40 pages plus appendices.  Then came the process of getting the handbook approved by the agency, which seemed to take forever because some new official forms were involved, even though the workshops had already smoothed the way for approval.  The handbook was eventually published in early 1987.  Then there were some requirements involving planning that we had to follow up on.  There was a fair amount of follow-up because there are always a lot of questions that come up when you issue a new handbook.</p>
<p>
<hi rend="italics">Q:  Did you have a sense of the impact of the handbook?</hi>
</p>
<pb facs="0030" n="Page0030"/>
<p>VREELAND:I don't think it had a lot of impact per se.  I think much of the impact the new handbook might have had had already been achieved through the workshops, except, of course, for those missions who had not had a workshop.  In general, the new handbook set into agency evaluation policy a collaborative approach and a management focus that had not been there before; it made some distinctions about types of evaluation that hadn't been so clearly laid out; and it required each new project to have an information plan to support improved performance by getting managers timely information on their project's progress and effects.  I think those were very important for the agency.  The terminology was clarified, definitions put in place; that was important.  I think that was a good step.</p>
<p>
<hi rend="italics">Q:  You did a video at that time did you not?</hi>
</p>
<p>VREELAND:  Yes.  It was my one and only experience with producing a video, and it was great fun.  We made the video to announce and publicize the new handbook; it wasn't a teaching video, it was more like a PR video, although it did point up a couple of themes, like the importance of having specific questions to guide an evaluation, and the need to gather data bearing directly on those questions.  The video was about 15 minutes long, and was played at the missions as part of the latest bunch of tapes they usually received from Washington.  The video depicted a scene in a Mission Director's weekly meeting, with the staff sitting around a table talking about evaluation.</p>
<p>
<hi rend="italics">Q:  You wrote the script for that didn't you?</hi>
</p>
<p>VREELAND:Yes.  The video itself was made professionallby an experienced outfit in northern Virginia.  By the mid '80s, Washington had become a little Hollywood in the video business along with New York and Miami.  There is a lot of video taping and movie making that goes on in this town.  There are some very good producers available, and we were able to get one.</p>
<p>
<hi rend="italics">Q:  The characters were AID people, was that right?</hi>
</p>
<p>VREELAND:No, they were all professional actors except for my husband who had recentlretired from his job in the government, so he was available to serve as an extra.  The others were professional actors playing the part of mission staff.  For that reason, my husband was not allowed to say anything.  The actor guild union rules are very strict.</p>
<p>
<hi rend="italics">Q:  What did he do?</hi>
</p>
<pb facs="0031" n="Page0031"/>
<p>VREELAND:He just sat at the meeting table as an extra.  He made facial expressions from time to time. That was amusing because as an amateur he tended to exaggerate the expressions he made.  He told me later that he gained a greater appreciation for the subtle expressions of good actors.</p>
<p>
<hi rend="italics">Q:  Well, what were some of the other projects you undertook in your work there?</hi>
</p>
<p>VREELAND:During that period, there was a fair amount of support for missions, both technical assistance and responding to queries about methods and planning and that sort of thing.  That continued to be part of my responsibility: supporting compliance with the handbook, especially the monitoring and evaluation components of new projects, annual evaluation plans and development of methodologies, making sure that evaluation requirements were integrated into the training courses for new entries and senior managers, managing a contract for synthesizing lessons learned from the annual crop of agency evaluations.  The impact evaluations continued, and I participated in the pre-evaluation team meetings before the teams took off for the field; these meetings usually ran several days, and part of that time was used to decide on methodologies that would be most helpful to the team.  I did some of that training for those teams.</p>
<p>
<hi rend="italics">Q:  Did you participate in any of these?</hi>
</p>
<p>VREELAND:I never got to participate in an impact evaluation while I was there.  The only evaluations I was involved with were in Egypt and Indonesia at the request of those missions.</p>
<p>
<hi rend="italics">Q:  What was that about?</hi>
</p>
<p>VREELAND:The Egypt projects had to do with aspects of information use.  One was a project that dealt with new “appropriate” technologies, enabling the scientific community in Egypt to access recent technical information in the United States or Europe or elsewhere.  Part of that was getting satellite access to relay this information, and how the process of accessing and distributing international scientific information would be managed in Egypt, at least initially, and where that responsibility would be housed in the scientific community.</p>
<p>
<hi rend="italics">Q:  What was your function in that?</hi>
</p>
<p>VREELAND:Part of my responsibility was to evaluate the experience and quality and capabilities of several Egyptian institutions to handle those kinds of information functions.</p>
<p>
<pb facs="0032" n="Page0032"/>
<hi rend="italics">Q:  How did you find working with the Egyptians?</hi>
</p>
<p>VREELAND:Very interesting.  Talk about politics, my goodness!  People say that Cairo is Byzantine and it really is.</p>
<p>
<hi rend="italics">Q:  How was that manifest to you?</hi>
</p>
<p>VREELAND:The institutions that might have been the most qualified were not the most politically well positioned to acquire the responsibility, and thus the resources, but it took a while to figure that out.</p>
<p>
<hi rend="italics">Q:  Do you remember the names of the institutions?</hi>
</p>
<p>VREELAND:I don't remember them clearly.  But, it became obvious that the political pull of the directors of some of the institutions might end up enabling them to acquire access to the resources of projects for which they were not as well suited as other competing institutions.</p>
<p>
<hi rend="italics">Q:  This was a big project?</hi>
</p>
<p>VREELAND:It was a fairly large project, but it wasn't one of the really big ones.</p>
<p>
<hi rend="italics">Q:  It was not information dissemination?</hi>
</p>
<p>VREELAND:Not solely;  these were scientific institution-building projects.  My focus was on the information side of the project, which was a fairly significant aspect.  I had a great evaluation team, two Egyptians.  One was actually an American, who had been born and raised in Egypt and was heading IBM's office in Cairo at the time.  The other was a scientist, a physicist, and university professor, really excellent person with lots of experience. They were really fine people.</p>
<p>
<hi rend="italics">Q:  Do you remember what you recommended or how it came out?</hi>
</p>
<pb facs="0033" n="Page0033"/>
<p>VREELAND:Most of the recommendations dealt with management improvements.I remember being very clear with the director of one of the scientific institutions about what was needed in the way of such improvements, and he was being vaguely threatening about going directly to the prime minister, who was a relative, if the mission took its resources elsewhere.  During the debriefing,  I could see that some of the mission staff present were getting a little fidgety and nervous.  I tend to be very clear and straightforward.  I would be courteous at all times, but I wouldn't mince words, and if there was a spade to be called, I usually called it.  I never had any problems,  no unfortunate feedback or fallout for the mission, but the mission was obviously a little anxious about what was being discussed.  They were very sensitive to potential political issues for obvious reasons.  The gentleman finally backed down and agreed with our recommendations.  I believe that if you place the argument on the grounds of what is going to be good for their country, there is hardly an argument left for them to make.  Now, how things worked out in the end, there was so much behind the scenes maneuvering going on, I never really knew.</p>
<p>
<hi rend="italics">Q:  This is a question of who gets the resources.</hi>
</p>
<p>VREELAND:Right.  You never really know, but what I got back from the mission subsequently was very positive and apparently things had worked out for the best.</p>
<p>
<hi rend="italics">Q:  Any other projects you worked on?</hi>
</p>
<p>VREELAND:Oh yes.  You know there was another Administrator who came in and there was another set of recommendations and statements to be made in memos for the Administrator's use.</p>
<p>
<hi rend="italics">Q:  This was Administrator Roskens?</hi>
</p>
<p>VREELAND:There was Roskens, and then there was Administrator Woods.  No, Woods first then Roskens.  It seemed that for each new admininstrator, there was a new evaluation “initiative”, and I was usually assigned the task of putting together one or more memos on the subject. Or provide support for related senior staff meetings.  I did attend Roskens' confirmation hearing, again on my own time, because I was interested in finding out what kind of questions the Senators werasking him about accountability, about effectiveness, about results and then what kind of answers he gave for which we had all done our best to prime him.  I remember being reminded yet again about this continuous concern in Congress about accountabliliy.  Are we having any results?  What is coming out of all of this foreign aid money?  Is AID managing well?  When diRoskens come in?</p>
<p>
<pb facs="0034" n="Page0034"/>
<hi rend="italics">Q:  It was after my time so it was '90.</hi>
</p>
<p>The beginnings of the PRISM initiative - 1988</p>
<p>VREELAND:Roskens was '90, so it was about the time Allan Woods was Administrator, 1987 to 1989.  It was really clear by the time Roskens became administrator that there were two key words that kept coming up in the discussions between the Agency and the Hill.  One was accountability and the other was the word results.  Earlier you could say that the key word was impact, but now the word was results.  Around that same time in 1987, correct me if I am wrong about this, Haven, the Development Fund for Africa was passed on the Hill, right?  I had been working closely with some folks in the Africa Bureau, especially Emmy Simmons and Cindy Clapp-Wincek, helping as much as I could in their efforts to support program monitoring and evaluation plans, and watching them shift the emphasis of these plans to what came to be known as “program performance information systems” — seeing if it was possible to get a sense of the overall results of a mission's entire portfolio of projects and other activities, not just of individual projects. When the Development Fund for Africa was passed by the Congress, the Africa Bureau had put a stamp on that piece of legislation that made the new development fund a very self consciously performance-based program.  The bureau wanted to build evaluation into the program as a very important part of the whole effort, and accepted the task of reporting periodically to Congress on the overall performance and results of the new program.  Then, in late 1988, I succeeded in getting myself seconded to the Africa Bureau for two months when Cindy, the person who handled the evaluation task, went on maternity leave.  This assignment gave me the chance to observe firsthand how the process of integrating evaluation and information on program performance into the actual operations of the bureau was working out.</p>
<pb facs="0035" n="Page0035"/>
<p>Now, as we watched the experience of the Africa Bureau unfold under the Development Fund for Africa, my colleague Gerry Britan in CDIE and I recognized that this approach — setting up management information systems that focussed on the performance of entire mission programs — was something that could be done throughout the agency.  It didn't have to be confined to one bureau.  A performance-based approach to planning and programming and operations and evaluation could be introduced systematically throughout the agency.  So in 1989 we started a second round of workshops that really focused on that.  You know, as I was thinking back on my career at AID, I recognize there were two main phases.  The first phase was working inside the agency using the agency's own qualities, skills, interests to make evaluation a useful and acceptable part of an AID officer's everyday tool kit.  The second was helping position the agency better to respond to what Gerry and I could sense was coming down the pike, which was going to be a more comprehensive demand on the entire Federal Government to pay attention to results and its performance.  One involved mostly working internally with the agency; the other, while still working internally, also involved networking outside the agency and staying alert to what was happening on the outside in a way that I hadn't before.  The two phases began to overlap in 1988 and certainly by 1989.  And it was the second phase that engaged me during my last several years of working in the Agency.  And as always we used any opportunities that came up to promote the new approach to evaluating program performance. There were always some opportunities early on in a new administrator's regime to capture attention for evaluation.  So, we continued to do that.  By 1990,  we had another series of workshops started which we called “pilots”, to train agency staff in new concepts like strategic objectives, and how to measure performance and results at the level of whole programs rather than individual projects, and we made these concepts — what we called “PRISM” — a key aspect of Rosken's “evaluation initiative” when he became admininstrator.  We continued working very collaboratively with agency staff in the bureaus and some especially receptive missions,  and then finally, I think it was in 1993, what we expected to happen did happen: what was called the Government Performance and Results Act (GPRA) was adopted by Congress, and all the earlier effort that Gerry and I had put into introducing the concepts of strategic planning, clarifying objectives, specifying performance indicators for showing success, results, concern for regularly collecting data on these...</p>
<p>
<hi rend="italics">Q:  That wasn't being done before?</hi>
</p>
<pb facs="0036" n="Page0036"/>
<p>VREELAND:It had not been done so systematically before at the level of whole programs as distinct from individual projects.  That was why the Africa Bureau's efforts under the DFA had been so unusual and groundbreaking in the agency.  The bureau was a great laboratory for testing a new approach, and Gerry and I worked closely with their staff and then moved to replicate the approach agencywide.  One of my favorite people in AID was Emmy Simmons.  Both intellectually and in terms of her commitment, she was responsible for a lot of the fuel that Gerry and I were able to draw on to move a new approach to measuring performance across the agency.  Much othe stuff about clarifying program objectives, deciding on program performance indicators, much of that was tested out first in the Africa Bureau and then gradually spread throughout the agency.By the time the GPRA hit the fan, terms like results, program performance, strategic objectives, strategic planning, performance indicators, these were not unfamiliar words in the Agency.  Missions and bureaus were already working with those concepts in very practical ways.  What did happen, however, which was somewhat disconcerting at first, was that another bureau was assigned responsibility for implementating GPRA in the agency.  That was the management bureau.  They proceeded to develop their own approach and set of requirements and a somewhat different terminology, so the two had to be melded together.  Their approach was related to Vice President Gore's “reinvention of government” or “re-engineering”, which helped guide the federal government'response to the GPRA.  So thewere re engineering, and we were doing strategic planning, and the two had to be brought together, and in a way that minimized confusion in the agency.  It took a while.  I left before the merging was completed, but I understand that the differences were eventually ironed out.</p>
<p>
<hi rend="italics">Q:  Was this embodied in the concept called PRISM.</hi>
</p>
<p>VREELAND:Yes.  CDIE started PRISM well before GPRA and reinvention/re-engineering.  The concepts that made up what we called the PRISM system were the ones that evolved in part from the Africa Bureau's initial experience in responding to the DFA, and in part in response to early intimations of congressional concern with program management and accountability in the executive branch, including the DFA, precursors to the GPRA that Gerry and I could detect in the late 1980s.  I believe Gerry had excellent management contacts throughout the government, and could see what was coming, but also influence it to some extent.</p>
<p>
<hi rend="italics">Q:  What is PRISM?</hi>
</p>
<pb facs="0037" n="Page0037"/>
<p>VREELAND:It was an acronym for Program Performance Information for Strategic Management, using the initials with a bit of poetic license.  It emphasized the use of information in strategic planning and management, using data on key indicators of progress and results of mission programs.  Using information was a very important part of that.  Strategic management was an important part of that.  PRISM operatemorat the program level than at the project level, but the two had to be meshed through the planning and reporting system.</p>
<p>We chose the name “PRISM” because the acronym for the earlier term, Program Performance Information System, that had evolved from the Africa Bureau experience was not an attractive-sounding acronym, and the agency was accustomed to using acronyms and initials. When the time came to move the new approach into the wholagency, Gerry and I realizewe needed a catchy name for it.  I remember waking up one morning and having one of those eureka experiences.  I rushed to the office —  I think Paula Goddard was still ouDeputy at that point — and I sat down in her office and said “I think I've got it! PRISM!”,  and I spelled out the meaning of the acronym.  So PRISM was the name we used until the beginning of the re-engineering effort under the management bureau.  They had different terminology they wanted to use, so the PRISM acronym was eventually consigned to history.  The point was that the concepts and the system and the approach of re-engineering and GPRA were fundamentally the same as those CDIE had been promoting since 1989 through the “pilot” workshops in the field and conferences in Washington; they all shared a focus on getting results and measuring performance.</p>
<p>
<hi rend="italics">Q: What were some of the basic concepts?</hi>
</p>
<p>VREELAND:One was the concept of strategic planning which was quite different from the long-term planning that the Agency had practiced before.  Long-term planning is pretty much straight-linor linear planning, looking ahead to the future and the factors you are likely to have to deal with based on what you know.  Strategic planning is much more flexible.  It is a concept that you actually live in a world of uncertainty and conflict and difficult choices, and you position your mission or your company — the strategic planning concept came from the private sector —  you position your company to look out for and take advantage of changes as they come up and adjust your plans accordingly.  It is a dynamic, continuous planning process.  In theory if you were doing a long-term plan, you could sit down on day one and put together a plan out to year ten and that would be it.  You can't do that in strategic planning; it is a continuous process.  You have to look at the plan periodically, like every six months, at most a year, and revise, assess your performance and revise again and so forth.  That is one concept.  The other is being really clear about your objectives and specifying them very precisely in terms of some indicators of success.</p>
<p>
<hi rend="italics">Q:  That is when you use the objective tree approach?</hi>
</p>
<pb facs="0038" n="Page0038"/>
<p>VREELAND:Right, up to a point.  Thinking through an “objective tree” is a useful technique, but to work, strategic planning also has to be a very collaborative process.  You can get a bunch of experts together and do a long-term plan; you can also get a bunch of experts together and work up an objective tree.  That's fine.  In strategic planning, however, you have to have a fairly large number of stakeholders or constituents involved in setting objectives and developing the plan, making it work and revisiting it and making it work better.  It is time consuming.  It is a very involving kind of process.  The payoff is, of course, enormous, and that is why people are willing to spend time and resources on the process.  But, no question about it, it is intensive and time consuming.  I am not sure that, at least up to the time I left the agency, all our missions were willing to devote that amount of time.  And, at least initially,  the process certainly wasn't collaborative with the host country or other possible stakeholders like the State Department or Treasury or even the Defense Department; I was willing to accept unilateral mission strategic planning at the beginning, for the practical reason that our own staff had to become familiar with the process before engaging counterparts and other stakeholders.  But if the process remained unilateral, then I think it would become ineffective and eventually pro forma.</p>
<p>
<hi rend="italics">Q:  Were there some other elements to this?</hi>
</p>
<p>VREELAND:Specifying indicators, again.  This took a lot of skill and thought on the part of the agency.  It is very easy to slip up on the indicator side of strategic planning.  What you are looking for are indicators that will tell you if you are heading in the right direction during program implementation, and help you begin to take account of actual results as you move along.  It would be very easy to fall into the trap of selecting indicators for which you could never get the information because it would be too difficult or expensive to collect the necessary data, or to select indicators for which the timing is wrong because you are looking for a result much too early in a program, or much too late.  Selecting indicators is a skill and requires a balance between a vision of development possibilities and a practical sense of development probabilities. You really have to think through what you want to achieve in a program, and do that very carefully, and get a lot of input so that what you select as your final indicators of success, as well as interim indicators that you will monitor as your program proceeds, will be significant — they will tell you something useful about how well your program is going.</p>
<p>
<hi rend="italics">Q:  Did each mission come up with their own or did CDIE introduce some guidance on this?</hi>
</p>
<pb facs="0039" n="Page0039"/>
<p>VREELAND:  When Gerry and I first started working on PRISM, we strongly encouraged missions to come up with their own indicators because we wanted PRISM to be useful to the missions, so they would be committed to making the system work.  By 1990, our watchwords were “managing for results”, so the system had to focus first and foremost on actually getting results and on being useful to managers for this purpose.  You would never find anyone investing any time or money in anything such as data collection unless it were going to be useful to them.  So, we emphasized that.  Needless to say, the regional bureaus also pushed for a decentralized indicator system, in which each mission could develop its own performance indicators and measures.  But there was always a push, especially from the Administrator level, for getting more comprehensive picture of the agency's results:  Can't you tell me how we are doing across the board?  Can't you give me some hard information on our impact?  How are we doing generally in promoting improved child health around the world?  This push really intensified under GPRA.  We had to get some agency-wide indicators into the system.  The related question was: who was going to do that?  Who was going to spend the money on collecting data on agency-wide performance indicators? As I recall, we struck a compromise.  CDIE, with advice from or together with technical bureau offices,  would suggest to the missions that they use some agency-wide or “standardized” indicators, and we in CDIE would pull the data together so that there would be an agency-wide picture of performance and results available once a year.  I think the missions respected Washington's need for that kind of information, within reason.How the compromise worked out in the final analysis I don't know.  I left before the matter was fully resolved.  It is not an insoluble problem, but there are limited resources for all of this, and the information needed by managers in the field was not always the same as the information needed to report on overall agency performance to Congress.  There is also the question of who is going to be responsible.</p>
<p>
<hi rend="italics">Q:  So most of the reinventing government stuff happened after you left, is that it?</hi>
</p>
<pb facs="0040" n="Page0040"/>
<p>VREELAND:The GPRA came along in 1993, and Gore'reinvention effort started off around the same time, during his National Performance Review; this was about two years before I left AID.  The federal agencies began to be formally notified that there would be changes in requirements for reporting on their strategic plans and performance indicators.  I think the Vice-President's reinvention effort went beyond the Government Performance and Results Act because Gore envisioned a government that would be truly much more efficient and responsive to citizens and clients than I think GPRA envisioned.  GPRA was more focussed on accountabliliy for results.  Gore's reinvention was more comprehensive; he really wanted to turn the government around in a fundamental way,  make it closer to the American people, the “clients”.  That is my interpretation, although the two efforts were of course very closely related.  At any rate, I spent most of my time working on implementing GPRA until my retirement in 1995.  We prepared a number of cables that went out.  By then John Eriksson was the director of CDIE.  Now you have to understand that PRISM was itself an invention, and we had to make it up as we went along from 1988 to 1993, learning as we went.  Even when the “pilots” became more formal PRISM workshops on “program monitoring and evaluation”, and even as training in PRISM concepts began to be integrated into the agency's regular management training courses, we still hadn't gotten to the point of producin“official” PRISM guidance or handbook, similar to the evaluation handbook. It was time to get out some formal guidance: this is what PRISM is; this is what it's supposed to do; this is what you are required to do.  So, we began issuing a series of cables that spelled out the guidance and the policy on strategic objectives, on performance indicators, on strategic planning, how plans would be reported, and related terms, standards and definitions.  Then, when the Management Bureau took on re engineering, they adopted a different approach to guidance: not a handbook, but rather a series of computer-available guidance papers.  I worked on some of those, but the task had not been completed before I left;  I believe much of my earlier work in defining concepts, standards and terms in cables to the field was ultimately superceded by the guidance issued by the management bureau, just as the term PRISM itself.</p>
<p>
<hi rend="italics">Q:  What happened to the evaluation function?  What was going on there?  Did you have any involvement in what was being evaluated?</hi>
</p>
<p>VREELAND:The program continued.  I recall that Janet Ballantyne gave renewed priority to the impact evaluations.  But I had my hands full with PRISM, so I don't recall specifics of CDIE's evaluation studies at the time.</p>
<p>
<hi rend="italics">Q:  She was just before John Ericksson or just after John Ericksson.</hi>
</p>
<pb facs="0041" n="Page0041"/>
<p>VREELAND:Both. She headed CDIE twice.  She was there when we began PRISM, and I recall she gave Gerry Britan and myself strong encouragement; then she was appointed Director again just before I left the agency. There was a cut in budget at some point, whethe central evaluation function was cut back.</p>
<p>
<hi rend="italics">Q:  Roskens increased the funding so it had to be after Roskens when it was cut.</hi>
</p>
<p>VREELAND:But the function continued.  And the development information service continued.</p>
<p>Q:  Did you see any linkage between the evaluation function and the information function or did they just happen to be in the same office?  How did you feel that the two related to each other?</p>
<p>VREELAND:I always thought they were closely related in some practical ways.  And, of course, the whole PRISM concept tied information use together with evaluation as one aspect of performance assessment.  I know that the researchers on the evaluation side used the information system as a service in obtaining relevant documents that they wanted to review in designing their evaluation studies and preparing for fieldwork.  Also, the evaluation reporting system — I think it was a never-ending effort — had to make sure that all the evaluation documentation produced in the agency got into the information system.  I think we managed to capture most of it.  We set up the distribution system sufficiently well that relevant parts of the agency received evaluative documentation from the field —  trip reports, evaluations, implementation reviews, anything that would have substantive information about the programs' implementation and success or failure, we managed to capture fairly systematically.  Also, and I think it was strongly emphasized during your, Haven North's, tenure as CDIE director, we attempted to share our information widely within the donor community and conversely capture some othe information on other donor experience in our system, so that if someone wanted to look at, say, what has been the experience with rural roads projects, what has been our experience with private investment development projects, they would have good access to a much wider body of experience.  That whole DAC relationship was one that I only got involved with really closely when you came in to head up the center.  It was at that time that I began preparing some reports for them and actually participating in some of the seminars held by the OECD DAC Expert Group on Aid Evaluation.</p>
<p>
<hi rend="italics">Q:  You were in the seminars?</hi>
</p>
<p>VREELAND:I went to the Africa regional seminar in Abidjan, which was very successful and a lot of fun.  I was going to one in Kuala Lumpur in 1992, in fact it was a seminar that I was responsible for organizing, but at that time my husband died and I was not able to attend.</p>
<p>
<pb facs="0042" n="Page0042"/>
<hi rend="italics">Q:  What were these about?</hi>
</p>
<p>VREELAND:These were generally about different aspects of the role of evaluation in foreign aid, and the experience of donors and recipients in the evaluation process.</p>
<p>
<hi rend="italics">Q:  With other donors?</hi>
</p>
<p>VREELAND:With other donors.  The regional seminars were held sequentially in different parts of the world so that representatives of both recipient and donor governments, as well as the multilateral development banks, could attend.  I think they were very useful.</p>
<p>One seminar that never took place and which was one of my earlier efforts for the DAC expert grouwas in 1989.  That year the American Evaluation Association was having its annual meeting in San Francisco; the theme of the conference was the international dimensions oevaluation, and I had agreed to serve as program chair for the conference.  I had lined up a stellar list of speakers and panelists, including the highly-respected Inspector General of the Department of Health and Human Services, and Bill Fuller, whom I had first met when he was Mission Director in Indonesia and was then head of The Asia Foundation.  CDIE invited the members of the DAC expert group on evaluation to attend the conference, and several had decided to do so; I organized some special meetings for them during the conference.  Well, that was the year of the big 1989 San Francisco earthquake, 7.1 on the Richter scale.  Some of our invitees managed to get there before the earthquake struck, some did not; most, including Janet Ballantyne, were forced to return to Washington.  For those who did make it, we put together a rump program; and I think they enjoyed the whole experience.  It certainly was an experience for me!  I remember the expert group delegate from the Netherlands and I got completely fed up with all the aftershocks and rumbling.We decided to take a break and take a long ferry ride across the bay to Sausalito.  It was a blessing.  She and I both loved the water, and right then the water seemed a lot firmer than the land.</p>
<p>
<hi rend="italics">Q:  This is Hedy von Metsch.</hi>
</p>
<pb facs="0043" n="Page0043"/>
<p>VREELAND:Yes.  Whenever we saw each other after that, we would always recall that meeting; it was a very bonding experience.  There was another footnote to that experience.  You know, when you're sitting in the top floor of a hotel, and the walls are shaking and the building is swaying and you are wondering if it's going to collapse and take you down with it, one's priorities suddenly change.  Life suddenly seems a whole lot shorter, and if you've got anything to say it's time to say it.  Well, that year was also the beginning of the breakup of the old Soviet Bloc.  In an earlier job working with the Army, I had studied the many nationalities and ethnic groups around the periphery of Russia, the old Soviet Union, and I could see that as soon as the lid was off — the lid that Moscow had put on these seething animosities — it was likely that all hell would break loose, and that would not necessarily be in the best interests of the US and Europe.  I thought it might be a good idea if the US got in there fast with some intensive training in conflict resolution or conflict management, and even more ambitiously, take new and old leaders, even large parts of populations, through formal visioning exercises to help them get off their past and invent a different future, before it was too late.  So, when I returned to Washington after the earthquake, I went to see Rich Bissell, who was the only person I could think of who might be sympathetic to the idea, and he suggested I put together a short proposal and that he would walk it past some of his colleagues at State.  A few days later I asked him what had happened, and he laughed and said, in effect: “When they finally scraped themselves off the floor, they said they would take it under advisement.”Of course, that was the last I heard of it — obviously much too radical, and State was not about to let AID into that part of the world at the time.  A couple of years later, we did have programs in Russia and Eastern Europe, mostly in the areas of market reforms and privatization .</p>
<p>
<hi rend="italics">Q: What about the development information function.  Was there any sense about how it was being used by the agency or anybody else?</hi>
</p>
<p>VREELAND:The development information function.  By then I was a little out of touch with the development information system but I'm assuming that it was being used.  The availability of computer capabilities to staff in Washington and in recipient countries and the ability of the development information service to expand its resources beyond our AID library to the libraries of other institutions, those two things alone undoubtedly greatly increased the use of the development information (DI) service.  People could access the DI directly.  The DI division of CDIE prepared CD's that they could send out to missions so that staff could sit down access information directly; they didn't have to go through Washington.  It was like a revolution.  I'm assuming there must have been a big expansion in use.</p>
<p>
<hi rend="italics">Q:  Were there any specific things that you were involved in during this last period you were talking about?</hi>
</p>
<pb facs="0044" n="Page0044"/>
<p>VREELAND:There were a couple of things.  As you know there had been a shift in the program.  Elliot Berg's report that came out in 1980 was followed by the Reagan administration's shift to a strong emphasis on privatization and development of the private sector which was, in turn, reflected in AID's program.  There was a very substantial change in the agency's program during the 80's.  By the end of my stay in the agency, however, I think a more balanced program had been established.</p>
<p>
<hi rend="italics">Q:  Balance between what?</hi>
</p>
<p>VREELAND: A balance between promoting the private sector, that is, efforts to change overly centralized command economies into market economies, and support for institutions maintaining basic public services like health, including maternal and child health threough family planning,  and education.  For a while during the '80s, it seemed as though the role of the public sector in development might be discounted altogether.  I welcomed the balance between private and public roles; I had become heartily sick of the unqualified adulation of the “private sector” by the end of that decade, but I think by the early '90s AID had achieved more of a balance.  I also think that Peter McPherson, in spite of his being the embodiment of the ideological shift toward the private sector and despite all the ads I saw in the local subway, actually managed to protect the population program in some important ways.  By the way, I should add that a balanced program also ran the risk of spreading our limited foreign aid resources too thinly over too many types of programs; that was a theme that kept coming up toward the end of my stay, and I'm not sure that the problem was ever resolved.  At any rate, these conditions — the emphasis on programs related to developing the private sector, and then a more disparate program that tended to defy efforts to focus on a few key strategic objectives — presented difficult challenges to our work on performance indicators and strategic planning, and a lot of my time was spent on those concerns.</p>
<p>
<hi rend="italics">Q: Were there other things?</hi>
</p>
<pb facs="0045" n="Page0045"/>
<p>VREELAND:Yes.  In the early '90s, we mounted a major technical assistance effort in conjunction with PRISM.CDIE issuea large contract, one of the largest contracts I've ever managed.  Through this contract, we made technical assistance available to missions to support their strategic planning and related data collection requirements.We could also make technical advice on strategic planning available to counterpart institutions;  I don't believe we did so at least up until I retired, although I did attempt to work closely with other agency programs, like the one managed by Jeanne North, that focussed directly on institutional development and management improvement in recipient countries.  We also captured and maintained data on the performance indicators that missions were using to measure their progress, so that we could generate profiles of mission performance and assess where the agency stood at any given time in terms of the strategic objectives it expected to accomplish. These profiles were part of an effort to prepare an annual report for the Administrator on the progress and results of the agency's program.  Preparing the first of these annual reports was a very difficult task; I understand that they have become increasingly standardized, and are now a routine part of the agency's system for reporting on its performance under the GPRA.</p>
<p>
<hi rend="italics">Q:  Why was it so difficult?</hi>
</p>
<p>VREELAND:Oh, for many reasons.  First, the programs themselves were often in transition, from being a diffuse collection of disparate projects to becoming more focussed on a few major strategic objectives.  At first, missions sometimes tried to conceal their diffuse project portfolios behind vaguely defined strategic objectives or high-level objectives that were expressed much too ambitiously given the assistancresources available.  So that was one reason.  Second, there were practical issues regarding the quality and timeliness of the data that missions were reporting. To what extent could we trust the quality of a survey or a statistical report in a developing country?  Then there was the matter of interpreting what the data meant,  the significance of the data.  Having the raw data on performance indicators was never enough.  Ithe data showed improvement, to what extent was it attributable to the mission's program, other donor programs or other factors like the weather?  Ithe data showed a deterioration or simply stayed the same from year to year, did that really mean the mission's program was a failure?  How long should we wait for a change in the indicators: three years, four years?  And what should we say about performance in the meantime?  Perhaps the mission had set too high a standard of performance for itself, given the political factors I mentioned earlier.  While we didn't want to explain away genuine failures, at the same time we wanted to cast what we'd accomplished in a reasonable light in a real-world context.  So we had to work very closely with the bureaus and missions on interpreting the meaning of the data they were reporting.  Trying to pull all this together was a real challenge for the first couple of years.  I think by now people have gotten a bit more comfortable with the process.</p>
<p>
<hi rend="italics">Q:  What are these reports based on?</hi>
</p>
<pb facs="0046" n="Page0046"/>
<p>VREELAND:All the data and evaluative stuff the missions sent in.</p>
<p>
<hi rend="italics">Q:  Based on their strategic plan?</hi>
</p>
<p>VREELAND:Yes.  The data they were collecting on their performance indicators, plus theimajor evaluation findings, and the findings of the central evaluations carried out by CDIE, so it was a combination of information.  Then another element of this major contract was this provision of technical assistance.  We wanted to cover every mission in the world, take them through a strategic planning exercise for the first time and maybe a second time, get them accustomed to doing strategic planning on an annual basis, helping them with indicator development, helping them with whole data collection matter, sources of information, setting up their sources.  That was a major part of my work in 1994.</p>
<p>Through connections I developed outside the agency through the American Evaluation Association and the National Performance Review, especially when Gerry Britan became associated with the latter, and then the reinvention process and the GPRA, I was invited in late 1994 to participate in a federal agency-wide effort to prepare a handbook of guidelines for senior administrators in all federal departments and agencies on implementing the GPRA.  So, in that sense, it was nice to wind up my work at AID on a government-wide level.  It also positioned me to bring back to the agency first-hand information on what the current thinking was throughout the federal government, so that as our own reinvention effort proceeded, we could be in fairly close tandem with other federal agencies.  Also, given AID's earlier experience with strategic planning since 1989, I was able to make some useful contributions based on AID's practical experience with some of these planning and management techniques.  I was pleased to see, by the way, a couple of days ago in the paper an article about a debriefing or event on GPRA with the Senate, where AID was one of the few federal agencies that got a passing grade.  I was delighted to see that.  I think it says something for the program as well as the agency's effort to present the program and its accomplishments.  Well, I guess that winds up my work in AID...</p>
<p>Observations on the role of evaluation</p>
<p>
<hi rend="italics">Q:  Well let's wind it up with general remarks and you can add things later.  First, how would you characterize the evolution of evaluation from your early days to the present time?  Has it changed very much or is it the same thing?</hi>
</p>
<pb facs="0047" n="Page0047"/>
<p>VREELAND:Its probably pretty much the same thing; in other words, evaluations are carried out pretty much the same way as when I arrived in the agency, except for the central evaluation studies done by CDIE, including the impact evaluations.  What's changed a little is people's willingness to gather and use information about what's happening in a project or program, to improve its management or even to change the project or program.  I think that is a major change, a very important shift.  The problem of “utilization” of evaluation or performance assessment has always been the key issue, and not just in AID.</p>
<p>
<hi rend="italics">Q:  Is it more accepted now than it used to be?  I mean people are not so hostile.</hi>
</p>
<p>VREELAND:Right.  Not quite so hostile, and in some cases agency staff have swallowed the whole package of performance assessment, which is one aspect of evaluation.  I was stunned to read some stuff about a year after I left AID;  it was as though a whole new vocabulary, a whole new way of doing business had been acquired by the agency.  I have to say that I think the reason for that is that almost everybody came into AID wanting to make a difference.  It is that kind of an agency with that kind of a mission, and tends to attract people committed to equitable development in the world.  So, people were simply waiting for someone to come along and say: “Do it.”  “Find out if you really are making a difference, and if you aren't, then do something about that, too.”  In a sense, the GPRA stood as a counterweight to those political factors I mentioned earlier that were so often a constraint on making a program succeed.</p>
<pb facs="0048" n="Page0048"/>
<p>I don't want to overstate this acceptance.  I doubt that evaluation is any more popular today than it was; it's still seen as a disagreeable requirement.  Also, it takes time.  Especially if you have an external evaluation team come in who don't know beans about the country or beans about the project to begin with, it will be a pain in the neck and probably won't be very useful to a mission; none of that necessarily changed.  I think there may be more interest in collaborative evaluation where, if the promise of a payoff in better program management or better results is big enough, people will be willing to spend the time to do the evaluation right.  But, frankly, the big constraints to evaluation, collaboration, participation, acceptance, and use —  making evaluation really work for you in your jo—  are the time the evaluation process takes. And people are just so busy.  The administrative environment, the daily chores that you have to handle as an AID officer in the field (I may be wrong because I have been out of the agency for three years),  the time spent on things like preparing for audit, tracking project financial flows, designing follow-on projects, mediating conflicts with contractors, all those time-consuming tasks that you always have to deal with are still there.  They have not gone away.  Preparing vouchers, source requirements for the goods you import, I don't hear that any of that's changed.  When I was on that assignment ithe Africa Bureau in 1988/89, I went around to bureau staff who had just recently returned from the field and asked them about how they generally spent their week working in a mission: Roughly how much time did they spend on handling legislative requirements?  On financial accountability?  How much time did they spend with their counterparts ironing out problems with contractors, with miscommunication, with misunderstandings, with project equipment held up in customs, and the like?  Roughly how much time did they spend with their counterparts working on substantive issues about the program like site visits or designing and participating in an evaluation or sharing ideas or exchanging information or working around political constraints?  And, roughly, how much time did they spend on their owpersonal development, reading, thinking, catching up on their fields of expertise, that kind of thing?  Of all those elements, the routine requirements took up almost 100% of people's time. Working with counterparts on substantive project or program matters was hardly ever touched.  And as for personal development, zero time;  as one officer told me: “I do it on weekends, and that's what I call burnout time.”  I don't see that any of that has changed, reinvention, re-engineering or not.  In those kind of circumstances, it's very easy to slip into pro forma behavior.</p>
<p>
<hi rend="italics">Q:  Do you think that evaluation during your time had any impact on improving the quality of the program?</hi>
</p>
<pb facs="0049" n="Page0049"/>
<p>VREELAND:Yes, I do.  I sensed especially in the area of sustainability, the lessons about why some result of a project was not sustained, that these lessons found their way quite thoroughly into a number of programs.  I think the concept of collaborative management is something that has worked its way into our assistance programs.  But I also think there are limits, partly because of a tendency to change program directions before an earlier approach was fully tested and experimented with and evaluated; such shifts may occur for several reasons — technological improvements, or budget cuts, or ideological changes in US political leadership.Also, I think there are limits to the ability oevaluations to improve programs because some key questions or issues don't get looked at systematically in evaluations.  These questions deal with political, social or cultural factor— the kinds of factors about which a technician might say, “Well, those are the kinds of issues we'll work out when we implement the program.” Well, you know, that attitude definitely runs the risk of reinventing the wheel.  But, in general, I think that at least in a given country a project that is a follow-on project willy-nilly learns from experience with the previous project because you are continuing to work in the same country.  One contribution of CDIE's impact evaluations was the pulling together of similar experiences in different countries that exposed the political, social and institutional problems — and lessons — likely to be associated with particular types of projects.</p>
<p>
<hi rend="italics">Q:  Well you have had a chance to observe from a very good vantage point the impact of US foreign assistance.  Do you think it has been positive or constructive on the international level?</hi>
</p>
<p>VREELAND:Gee, that's a hard one for me to asnwer, because I guess my view of development is somewhat idiosyncratic.  You know, development can be a short-term push, command-led, force fed, or it can be a long-term effort.  It was certainly a long-term process in the United States.  The evidence we have of forced-march development is not particularly optimistic; at least, I don't think it is.  I also think there has to be a balance between the private and public sectors.  Countries that went overboard in one direction, command economies, I don't see any of them doing very well today, and most have been disasters.  Conversely, I look at developing countries in the Third World that were self-consciously private sector-based, yet were so corrupt, non-egalitarian, full of nepotism and just plain greedy that their economies were not going to thrive in the long run.  For example, the collapse of the Asian miracle was no surprise to me.  I had intensively studied some of these countries in the handbook program at American University, so it seemed that after the first flush of rapid development, these countries still lacked important civil and institutional and regulatormechanisms, so they could easily excuse accumulated bad judgments about investments and lacked anaccountability, and at some point without those mechanisms and a willingness to enforce them, those economies were going to go down, and they did.  So, the other extreme is no good either.  It takes a balance.</p>
<pb facs="0050" n="Page0050"/>
<p>I guess in thinking through your question that the real investment in development is in education.  To me, you can't go anywhere in equitable, sustainable development without education.  And the agency failed to keep that a high priority in its assistance program throughout the period I was there.  Yet the United States' own experience should have given a positive...</p>
<p>
<hi rend="italics">Q: Are you talking about formal education?</hi>
</p>
<p>VREELAND:Literacy, numeracy, basic education.  An educated population is absolutely essential for genuine development.  Yet, I think that AID was quite successful, almost despite itself.  This was because one way of contributing to development is to open up possibility for people where they didn't see the possibility before you arrived on the scene.  To me, opening up people to new possibilities is what development is all about.  Then if they are in a position to seize the opportunities...</p>
<p>
<hi rend="italics">Q:  Do you have some examples of some area where that might have taken place?</hi>
</p>
<p>VREELAND:  I remember reading about a series of meetings, I think it was in the health area, where people from different countries in Africa were talking with each other and sharing experience.  They all went home and made some crucial and effective changes in their own countries.  Among them, they had generated a bunch of new possibilities that they hadn't heard before or hadn't understood before or didn't grasp the significance of before.</p>
<p>
<hi rend="italics">Q:  How about areas like family planning, environment, things like that?</hi>
</p>
<pb facs="0051" n="Page0051"/>
<p>VREELAND:Yes, those are areas also.  I remember there was this alternative energy project that I was looking at in a village in Egypt.  They were using animal manure from the barnyards in back of houses in a poor village along the fringe of the desert to generate methane gas and piping the gas into the kitchens.  The woman of the house turned on the gas burner, and looked at this woman's face when the burner lit.  She was so proud; she was just delighted.  Now, it turned out that she was probably not going to use this new technology after all, because the Egyptian government was heavily subsidizing the propane gas tank sitting outside her house.  Yet that didn't seem to minimize her delight.  As long as she had donkeys in her barnyard, she would have gas for her stove.  There were many small instances like that throughout my visits to some dozen countries while I was working for AID:  you saw a light turn on in people's eyes.  It was something that had not been there before; they had not even dreamed of the possibility.  And once it had started, that sense of possibility seemed to expand.  I recall one of the findings of an early series of impact evaluations of rural electrification projects — that people in remote towns for whom electrification became possible realized that it might also be possible to increase the size of their businesses, to improve health facilities, improve local schools, which in turn opened up the possibility of organizing politically to lobby for such improvements, and so on.</p>
<p>
<hi rend="italics">Q:  How did you find the Agency as an organization to work for?</hi>
</p>
<p>VREELAND:Great.  I never met anybody in AID I didn't like.  There were some people who were more arrogant than others, but by and large, good willed, smart, experienced people.  The only handicap was this political one where sometimes you found the ground cut out from underneath you.  But nobody was doing it with malice or ill will; it was just part of the job environment.</p>
<p>
<hi rend="italics">Q:  Anything else you want to add at this point?</hi>
</p>
<p>Special assignment reviewing the Multilateral Development Banks'evaluation system - 1996</p>
<pb facs="0052" n="Page0052"/>
<p>VREELAND:  Not about my experience in AID.  Since I retired from the Agency, I have pretty much closed that part of my life and opened up another chapter.  As I said, I was given the great opportunity to end my career in the agency on a federal inter-agency level.  Then, shortly after I retired from AID in April 1995, I took the final step and ended my career in international development on a global level:  I took a four-month consultancy with an international task force that was looking at the future role of the multilateral development banks; my own participation on this task force was sponsored by the World Bank.  My function was to support an examination of the role of evaluation in the multilateral development banks, their operations and management, as well as in the development and performance of their lending programs.  I visiteall four regional development banks.  Two banks, of course, are here in Washington, the World Bank and the Inter American Development Bank.  I also visited the Asian Development Bank, the African Development Bank and the European Bank for Reconstruction and Development,  and interviewed key members of their managements and staff.  It was a wonderful experience, and I was able to make my final statements about evaluation on a global scale, so to speak.</p>
<p>
<hi rend="italics">Q:  What conclusion did you come to on the evaluation systems of these institutions?</hi>
</p>
<p>VREELAND:Very much like the conclusions I reached on AID.  It was an underutilized function.  It depended very much on pressure from the head,  the top of the banks.  And it was perhaps even more politicized than in AID, of course, because of competing membership interests.</p>
<p>
<hi rend="italics">Q:  You don't think evaluation was more independent in the multi lateral than the bi lateral?</hi>
</p>
<p>VREELAND:I don't think so.  The World Bank was clearly taking the lead in supportina strong evaluation function as a central function in the bank, an independent function.  But, it was taking an enormous amount of effort and intentionality to make that work because there was real resistance to “too much” independence.  There was quite a ways to go.  But there was some good work being done as you know.</p>
<p>
<hi rend="italics">Q:  What was your task force recommending?</hi>
</p>
<pb facs="0053" n="Page0053"/>
<p>VREELAND:Overall strengthening of the function, emphasizing clarity in program objectives, monitoring progress necessary for achieving intended results, and learning from experience.A subheading in the final report summed it up rather aptly: “Striving for Results”.  We also suggested, as part of greater coordination among the banks in general, an effort to “harmonize” their evaluation criteria and practices.  Perhaps the most controversial suggestion was setting up an arrangement to continue monitoring the institutional performance and development impact of the banks as a group.  As I recall, the reason for this task force existing in the first place was pressure on the banks from the US Congress as well as other major donor countries to become more accountable and to look more carefully at their impact and results.  Here again were those key words “impact” and “results”, and the banks had to respond to that.  Now, that pressure is not going to go away, but it may become less effective.  To me, one interesting dimension of this matter of accountability was the extent to which the multilateral banks, those that have had high reflow or repayment rates, were becoming more self-sufficient;  they no longer depended as heavily on contributions from their country owneras they used to. They were reaching a position where they could sustain themselves at a certain level of operation. So unless their country executives became more alert to questions of bank performance, including comparative performance, the bank staffs could thumb their noses and avoid being accountable to anybody.  Tharisk concerned me because I wasn't sure that their programs were any more effective or successful than those of the bi laterals like AID.  They were different and larger, but not necessarily more successful.</p>
<p>
<hi rend="italics">Q:  Any other last thoughts?</hi>
</p>
<p>VREELAND:No, I guess that's it, and I really enjoyed this.</p>
<p>
<hi rend="italics">Q:  Well it was a great interview and I appreciate the opportunity.</hi>
</p>
<p>End of interview</p>
</div>
</body>
</text>
</TEI>